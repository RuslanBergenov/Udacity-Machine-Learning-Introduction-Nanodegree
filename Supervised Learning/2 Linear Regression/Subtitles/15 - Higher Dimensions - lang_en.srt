1
00:00:00,000 --> 00:00:01,399
So in the previous example,

2
00:00:01,399 --> 00:00:03,850
we had a one column input and one column output.

3
00:00:03,850 --> 00:00:07,084
The input was the size of the house and the output was the price.

4
00:00:07,084 --> 00:00:09,515
So we had a two-dimensional problem.

5
00:00:09,515 --> 00:00:12,519
Our prediction for the price would be a line and

6
00:00:12,519 --> 00:00:16,179
the equation would just be a constant times size plus another constant.

7
00:00:16,179 --> 00:00:18,079
What if we had more columns in the input,

8
00:00:18,079 --> 00:00:20,079
for example size and school quality?

9
00:00:20,079 --> 00:00:22,539
Well, now we have a three dimensional graph because

10
00:00:22,539 --> 00:00:25,304
we have two dimensions for the input and one for the output.

11
00:00:25,304 --> 00:00:27,429
So now our points don't live in the plane,

12
00:00:27,429 --> 00:00:30,804
but they look like points flying in 3-dimensional space.

13
00:00:30,804 --> 00:00:33,979
What we do here is we'll feed a plane through them instead of fitting a line,

14
00:00:33,979 --> 00:00:38,969
and our equation won't be a constant times one variable plus another constant.

15
00:00:38,969 --> 00:00:42,420
It's going to be a constant times school quality plus

16
00:00:42,420 --> 00:00:46,140
another constant times size plus a third constant.

17
00:00:46,140 --> 00:00:48,795
That's what happens when we're in three dimensions.

18
00:00:48,795 --> 00:00:51,179
So what happens if we're in n dimensions?

19
00:00:51,179 --> 00:00:56,524
So in this case we have n minus one columns in the input and one in the output.

20
00:00:56,524 --> 00:00:58,289
So, for example the inputs are size,

21
00:00:58,289 --> 00:01:00,829
school quality, number of rooms, et cetera.

22
00:01:00,829 --> 00:01:05,304
Well, now we have the same thing except our data lives in n-dimensional space.

23
00:01:05,305 --> 00:01:09,320
So for our input, we have n minus one variables namely; x_1,

24
00:01:09,319 --> 00:01:13,214
x_2 up to x_n minus one and for the output of the prediction,

25
00:01:13,215 --> 00:01:15,909
we only have one variable y hat.

26
00:01:15,909 --> 00:01:21,774
Our prediction would be an n minus one dimensional hyperplane living in n dimensions.

27
00:01:21,775 --> 00:01:26,090
Since it's hard to picture n-dimensions just think of a linear equation in n variables,

28
00:01:26,090 --> 00:01:32,070
such as y hat equals w1x1 plus w2x2 plus all the way to w_n

29
00:01:32,069 --> 00:01:38,269
minus one x_n minus one plus w_n and that's how we do predictions for higher dimensions.

30
00:01:38,269 --> 00:01:41,060
In order to find the weights w_1 up to w_n

31
00:01:41,060 --> 00:01:44,725
the algorithm is exactly the same thing for two variables.

32
00:01:44,724 --> 00:01:47,539
We can either do the absolute or square root tricks,

33
00:01:47,540 --> 00:01:50,625
or we can calculate the mean absolute or square errors,

34
00:01:50,625 --> 00:01:53,890
and minimize using gradient descent.

