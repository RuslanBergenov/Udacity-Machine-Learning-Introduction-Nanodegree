1
00:00:00,000 --> 00:00:02,250
对 答案是对数

2
00:00:02,250 --> 00:00:06,389
因为对数具有良好的特性

3
00:00:06,389 --> 00:00:11,929
即 log(ab) 等于 log(a) 和 log(b) 的总和

4
00:00:11,929 --> 00:00:13,294
这就是我们要做的

5
00:00:13,294 --> 00:00:17,559
我们得到乘积 使用对数

6
00:00:17,559 --> 00:00:21,854
现在得到因子对数的总和

7
00:00:21,855 --> 00:00:28,219
所以 ln(0.6*0.2*0.1*0.7) 等于

8
00:00:28,219 --> 00:00:35,700
ln(0.6) 加 ln(0.2)  加 ln(0.1) 加 ln(0.7) 从现在开始到这节课结束

9
00:00:35,700 --> 00:00:40,040
我们都要使用底数为 e 的对数 （自然对数） 而不是底数为 10 的对数

10
00:00:40,039 --> 00:00:41,759
底数为 10 的对数并没有太大区别

11
00:00:41,759 --> 00:00:44,945
所有原理都是相同的 它只是自然对数的结果乘上一个因子

12
00:00:44,945 --> 00:00:46,770
这样做只是一种惯例

13
00:00:46,770 --> 00:00:51,330
我们计算这些值得到 -0.51 -1.61

14
00:00:51,329 --> 00:00:58,164
-0.23 等 注意这些都是负数 这也是符合实际的

15
00:00:58,164 --> 00:01:01,560
这是因为 0 到 1 之间数字的对数都是负值

16
00:01:01,560 --> 00:01:05,594
因为对 1 取对数才能得到 0

17
00:01:05,594 --> 00:01:07,789
所以 概率的对数都是负值 对它取相反数是行得通的

18
00:01:07,790 --> 00:01:11,260
这样会得到正数

19
00:01:11,260 --> 00:01:15,740
这就是我们要做的 我们得到的概率的对数为负值

20
00:01:15,739 --> 00:01:18,905
对它们的相反数进行求和

21
00:01:18,905 --> 00:01:23,180
我们称之为交叉熵 这也是这节课中非常重要的概念

22
00:01:23,180 --> 00:01:25,385
如果我们计算交叉熵

23
00:01:25,385 --> 00:01:30,255
我们看到左侧错误的模型的交叉熵是 4.8 这非常高

24
00:01:30,254 --> 00:01:35,229
右侧的更优的模型交叉熵较低 是 1.2

25
00:01:35,230 --> 00:01:37,454
实际上这是一个规律

26
00:01:37,454 --> 00:01:38,810
准确的模型可以让我们得到较低的交叉熵

27
00:01:38,810 --> 00:01:43,185
而误差较大的模型得到的交叉熵较高

28
00:01:43,185 --> 00:01:44,629
这纯粹因为

29
00:01:44,629 --> 00:01:47,390
好模型可以给我们较高的概率

30
00:01:47,390 --> 00:01:52,599
它的对数取相反数后是个较小的数字 反之亦然

31
00:01:52,599 --> 00:01:55,250
这种方法比我们想象的还要强大

32
00:01:55,250 --> 00:01:59,180
如果我们计算出概率 并且得到每个点所对应的对值

33
00:01:59,180 --> 00:02:01,470
我们实际上得到了每个点的误差

34
00:02:01,469 --> 00:02:06,539
那么这里我们有两个模型的概率和概率的乘积

35
00:02:06,540 --> 00:02:09,944
现在我们对每个对数取相反数并求和

36
00:02:09,944 --> 00:02:15,319
如果我们把每个对值放到对应的点上

37
00:02:15,319 --> 00:02:17,859
我们可以得到每个点的值

38
00:02:17,860 --> 00:02:19,565
我们计算了这些值

39
00:02:19,564 --> 00:02:22,185
就得到了这些东西 来看一下

40
00:02:22,185 --> 00:02:24,319
如果我们仔细观察值

41
00:02:24,319 --> 00:02:26,430
可以发现这些分类错误的点的值较大

42
00:02:26,430 --> 00:02:31,295
如这个点是 2.3 另一个点是 1.6

43
00:02:31,294 --> 00:02:36,544
然而正确分类的点对应值都较小

44
00:02:36,544 --> 00:02:38,719
这个原因还是在于

45
00:02:38,719 --> 00:02:42,604
分类正确点的概率更接近于 1

46
00:02:42,604 --> 00:02:44,989
如果对这个概率的对数取相反数

47
00:02:44,990 --> 00:02:46,915
就会得到较小的值

48
00:02:46,914 --> 00:02:51,215
因此我们可以把这些对数的相反数作为每个点的误差

49
00:02:51,215 --> 00:02:53,539
分类正确点的误差较小

50
00:02:53,539 --> 00:02:57,594
而分类错误点的误差较大

51
00:02:57,594 --> 00:03:02,530
现在我们得到结论 交叉熵可以告诉我们模型的好坏

52
00:03:02,530 --> 00:03:06,800
所以现在我们的目标从最大化概率转变为最小化交叉熵 

53
00:03:06,800 --> 00:03:12,580
从而使左侧模型转变为右侧模型

54
00:03:12,580 --> 00:03:14,655
我们所寻找的误差函数

55
00:03:14,655 --> 00:03:17,000
就是这个交叉熵

