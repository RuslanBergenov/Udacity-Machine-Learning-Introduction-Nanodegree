1
00:00:00,000 --> 00:00:08,919
You now have an idea

2
00:00:08,919 --> 00:00:12,129
of how the many features of a survey or home,

3
00:00:12,130 --> 00:00:15,480
can actually fall into just a few main buckets.

4
00:00:15,480 --> 00:00:19,359
Because these buckets are explicitly in the data,

5
00:00:19,359 --> 00:00:21,698
we call them latent features.

6
00:00:21,699 --> 00:00:23,260
So the question is,

7
00:00:23,260 --> 00:00:28,179
how do we actually take all of the data we have from all of these features and

8
00:00:28,179 --> 00:00:33,644
condense it to only a few number of features that hold the most amount of information?

9
00:00:33,645 --> 00:00:38,859
One option, is that we could just choose a subset of the features for each

10
00:00:38,859 --> 00:00:44,134
latent variable that best captures the essence of that latent variable.

11
00:00:44,134 --> 00:00:46,394
Maybe the square footage of the home.

12
00:00:46,395 --> 00:00:49,200
Best captures the latent variable related to

13
00:00:49,200 --> 00:00:53,975
home size and we could drop these other features related to the number of bedrooms,

14
00:00:53,975 --> 00:00:57,635
the number of bathrooms, garages, etc.

15
00:00:57,634 --> 00:01:01,384
We could perhaps do this with the other latent features as well,

16
00:01:01,384 --> 00:01:03,579
but it seems like we're losing a lot of

17
00:01:03,579 --> 00:01:08,084
potentially helpful information by simply dropping all of these features.

18
00:01:08,084 --> 00:01:11,140
There has to be a better way we can reduce the number of

19
00:01:11,140 --> 00:01:14,189
features in our dataset without simply dropping all of them.

20
00:01:14,189 --> 00:01:17,694
Well, in fact there is a way we can preserve

21
00:01:17,694 --> 00:01:22,284
more of the information and one of the main methods for doing this, is PCA.

22
00:01:22,284 --> 00:01:25,479
The very topic of this lesson.

