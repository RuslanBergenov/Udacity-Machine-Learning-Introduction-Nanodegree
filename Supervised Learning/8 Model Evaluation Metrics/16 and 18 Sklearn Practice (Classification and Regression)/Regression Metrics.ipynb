{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston Housing Data\n",
    "\n",
    "In order to gain a better understanding of the metrics used in regression settings, we will be looking at the Boston Housing dataset.  \n",
    "\n",
    "First use the cell below to read in the dataset and set up the training and testing data that will be used for the rest of this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tests2 as t\n",
    "\n",
    "boston = load_boston()\n",
    "y = boston.target\n",
    "X = boston.data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[  6.32000000e-03,   1.80000000e+01,   2.31000000e+00, ...,\n",
       "           1.53000000e+01,   3.96900000e+02,   4.98000000e+00],\n",
       "        [  2.73100000e-02,   0.00000000e+00,   7.07000000e+00, ...,\n",
       "           1.78000000e+01,   3.96900000e+02,   9.14000000e+00],\n",
       "        [  2.72900000e-02,   0.00000000e+00,   7.07000000e+00, ...,\n",
       "           1.78000000e+01,   3.92830000e+02,   4.03000000e+00],\n",
       "        ..., \n",
       "        [  6.07600000e-02,   0.00000000e+00,   1.19300000e+01, ...,\n",
       "           2.10000000e+01,   3.96900000e+02,   5.64000000e+00],\n",
       "        [  1.09590000e-01,   0.00000000e+00,   1.19300000e+01, ...,\n",
       "           2.10000000e+01,   3.93450000e+02,   6.48000000e+00],\n",
       "        [  4.74100000e-02,   0.00000000e+00,   1.19300000e+01, ...,\n",
       "           2.10000000e+01,   3.96900000e+02,   7.88000000e+00]]),\n",
       " 'target': array([ 24. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,\n",
       "         18.9,  15. ,  18.9,  21.7,  20.4,  18.2,  19.9,  23.1,  17.5,\n",
       "         20.2,  18.2,  13.6,  19.6,  15.2,  14.5,  15.6,  13.9,  16.6,\n",
       "         14.8,  18.4,  21. ,  12.7,  14.5,  13.2,  13.1,  13.5,  18.9,\n",
       "         20. ,  21. ,  24.7,  30.8,  34.9,  26.6,  25.3,  24.7,  21.2,\n",
       "         19.3,  20. ,  16.6,  14.4,  19.4,  19.7,  20.5,  25. ,  23.4,\n",
       "         18.9,  35.4,  24.7,  31.6,  23.3,  19.6,  18.7,  16. ,  22.2,\n",
       "         25. ,  33. ,  23.5,  19.4,  22. ,  17.4,  20.9,  24.2,  21.7,\n",
       "         22.8,  23.4,  24.1,  21.4,  20. ,  20.8,  21.2,  20.3,  28. ,\n",
       "         23.9,  24.8,  22.9,  23.9,  26.6,  22.5,  22.2,  23.6,  28.7,\n",
       "         22.6,  22. ,  22.9,  25. ,  20.6,  28.4,  21.4,  38.7,  43.8,\n",
       "         33.2,  27.5,  26.5,  18.6,  19.3,  20.1,  19.5,  19.5,  20.4,\n",
       "         19.8,  19.4,  21.7,  22.8,  18.8,  18.7,  18.5,  18.3,  21.2,\n",
       "         19.2,  20.4,  19.3,  22. ,  20.3,  20.5,  17.3,  18.8,  21.4,\n",
       "         15.7,  16.2,  18. ,  14.3,  19.2,  19.6,  23. ,  18.4,  15.6,\n",
       "         18.1,  17.4,  17.1,  13.3,  17.8,  14. ,  14.4,  13.4,  15.6,\n",
       "         11.8,  13.8,  15.6,  14.6,  17.8,  15.4,  21.5,  19.6,  15.3,\n",
       "         19.4,  17. ,  15.6,  13.1,  41.3,  24.3,  23.3,  27. ,  50. ,\n",
       "         50. ,  50. ,  22.7,  25. ,  50. ,  23.8,  23.8,  22.3,  17.4,\n",
       "         19.1,  23.1,  23.6,  22.6,  29.4,  23.2,  24.6,  29.9,  37.2,\n",
       "         39.8,  36.2,  37.9,  32.5,  26.4,  29.6,  50. ,  32. ,  29.8,\n",
       "         34.9,  37. ,  30.5,  36.4,  31.1,  29.1,  50. ,  33.3,  30.3,\n",
       "         34.6,  34.9,  32.9,  24.1,  42.3,  48.5,  50. ,  22.6,  24.4,\n",
       "         22.5,  24.4,  20. ,  21.7,  19.3,  22.4,  28.1,  23.7,  25. ,\n",
       "         23.3,  28.7,  21.5,  23. ,  26.7,  21.7,  27.5,  30.1,  44.8,\n",
       "         50. ,  37.6,  31.6,  46.7,  31.5,  24.3,  31.7,  41.7,  48.3,\n",
       "         29. ,  24. ,  25.1,  31.5,  23.7,  23.3,  22. ,  20.1,  22.2,\n",
       "         23.7,  17.6,  18.5,  24.3,  20.5,  24.5,  26.2,  24.4,  24.8,\n",
       "         29.6,  42.8,  21.9,  20.9,  44. ,  50. ,  36. ,  30.1,  33.8,\n",
       "         43.1,  48.8,  31. ,  36.5,  22.8,  30.7,  50. ,  43.5,  20.7,\n",
       "         21.1,  25.2,  24.4,  35.2,  32.4,  32. ,  33.2,  33.1,  29.1,\n",
       "         35.1,  45.4,  35.4,  46. ,  50. ,  32.2,  22. ,  20.1,  23.2,\n",
       "         22.3,  24.8,  28.5,  37.3,  27.9,  23.9,  21.7,  28.6,  27.1,\n",
       "         20.3,  22.5,  29. ,  24.8,  22. ,  26.4,  33.1,  36.1,  28.4,\n",
       "         33.4,  28.2,  22.8,  20.3,  16.1,  22.1,  19.4,  21.6,  23.8,\n",
       "         16.2,  17.8,  19.8,  23.1,  21. ,  23.8,  23.1,  20.4,  18.5,\n",
       "         25. ,  24.6,  23. ,  22.2,  19.3,  22.6,  19.8,  17.1,  19.4,\n",
       "         22.2,  20.7,  21.1,  19.5,  18.5,  20.6,  19. ,  18.7,  32.7,\n",
       "         16.5,  23.9,  31.2,  17.5,  17.2,  23.1,  24.5,  26.6,  22.9,\n",
       "         24.1,  18.6,  30.1,  18.2,  20.6,  17.8,  21.7,  22.7,  22.6,\n",
       "         25. ,  19.9,  20.8,  16.8,  21.9,  27.5,  21.9,  23.1,  50. ,\n",
       "         50. ,  50. ,  50. ,  50. ,  13.8,  13.8,  15. ,  13.9,  13.3,\n",
       "         13.1,  10.2,  10.4,  10.9,  11.3,  12.3,   8.8,   7.2,  10.5,\n",
       "          7.4,  10.2,  11.5,  15.1,  23.2,   9.7,  13.8,  12.7,  13.1,\n",
       "         12.5,   8.5,   5. ,   6.3,   5.6,   7.2,  12.1,   8.3,   8.5,\n",
       "          5. ,  11.9,  27.9,  17.2,  27.5,  15. ,  17.2,  17.9,  16.3,\n",
       "          7. ,   7.2,   7.5,  10.4,   8.8,   8.4,  16.7,  14.2,  20.8,\n",
       "         13.4,  11.7,   8.3,  10.2,  10.9,  11. ,   9.5,  14.5,  14.1,\n",
       "         16.1,  14.3,  11.7,  13.4,   9.6,   8.7,   8.4,  12.8,  10.5,\n",
       "         17.1,  18.4,  15.4,  10.8,  11.8,  14.9,  12.6,  14.1,  13. ,\n",
       "         13.4,  15.2,  16.1,  17.8,  14.9,  14.1,  12.7,  13.5,  14.9,\n",
       "         20. ,  16.4,  17.7,  19.5,  20.2,  21.4,  19.9,  19. ,  19.1,\n",
       "         19.1,  20.1,  19.9,  19.6,  23.2,  29.8,  13.8,  13.3,  16.7,\n",
       "         12. ,  14.6,  21.4,  23. ,  23.7,  25. ,  21.8,  20.6,  21.2,\n",
       "         19.1,  20.6,  15.2,   7. ,   8.1,  13.6,  20.1,  21.8,  24.5,\n",
       "         23.1,  19.7,  18.3,  21.2,  17.5,  16.8,  22.4,  20.6,  23.9,\n",
       "         22. ,  11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], \n",
       "       dtype='<U7'),\n",
       " 'DESCR': \"Boston House Prices dataset\\n===========================\\n\\nNotes\\n------\\nData Set Characteristics:  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive\\n    \\n    :Median Value (attribute 14) is usually the target\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttp://archive.ics.uci.edu/ml/datasets/Housing\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n**References**\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\\n\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 1:** Before we get too far, let's do a quick check of the models that you can use in this situation given that you are working on a regression problem.  Use the dictionary and corresponding letters below to provide all the possible models you might choose to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  All but logistic regression can be used for predicting numeric values.  And linear regression is the only one of these that you should not use for predicting categories.  Technically sklearn won't stop you from doing most of anything you want, but you probably want to treat cases in the way you found by answering this question!\n"
     ]
    }
   ],
   "source": [
    "# When can you use the model - use each option as many times as necessary\n",
    "a = 'regression'\n",
    "b = 'classification'\n",
    "c = 'both regression and classification'\n",
    "\n",
    "models = {\n",
    "    'decision trees': c,\n",
    "    'random forest': c,\n",
    "    'adaptive boosting': c,\n",
    "    'logistic regression': b,\n",
    "    'linear regression': a\n",
    "}\n",
    "\n",
    "#checks your answer, no need to change this code\n",
    "t.q1_check(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 2:** Now for each of the models you found in the previous question that can be used for regression problems, import them using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models from sklearn - notice you will want to use \n",
    "# the regressor version (not classifier) - googling to find \n",
    "# each of these is what we all do!\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 3:** Now that you have imported the 4 models that can be used for regression problems, instantate each below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate each of the models you imported\n",
    "# For now use the defaults for all the hyperparameters\n",
    "dt = DecisionTreeRegressor()\n",
    "rf = RandomForestRegressor()\n",
    "ab = AdaBoostRegressor()\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 4:** Fit each of your instantiated models on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit each of your models using the training data\n",
    "dt.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "ab.fit(X_train, y_train)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 5:** Use each of your models to predict on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test values for each model\n",
    "pred_dt = dt.predict(X_test)\n",
    "pred_rf = rf.predict(X_test)\n",
    "pred_ab = ab.predict(X_test)\n",
    "pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 6:** Now for the information related to this lesson.  Use the dictionary to match the metrics that are used for regression and those that are for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right! Looks like you know your metrics!\n"
     ]
    }
   ],
   "source": [
    "# potential model options\n",
    "a = 'regression'\n",
    "b = 'classification'\n",
    "c = 'both regression and classification'\n",
    "\n",
    "#\n",
    "metrics = {\n",
    "    'precision': b,\n",
    "    'recall': b,\n",
    "    'accuracy': b,\n",
    "    'r2_score': a,\n",
    "    'mean_squared_error': a,\n",
    "    'area_under_curve': b, \n",
    "    'mean_absolute_area': a \n",
    "}\n",
    "\n",
    "#checks your answer, no need to change this code\n",
    "t.q6_check(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 6:** Now that you have identified the metrics that can be used in for regression problems, use sklearn to import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the metrics from sklearn\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 7:** Similar to what you did with classification models, let's make sure you are comfortable with how exactly each of these metrics is being calculated.  We can then match the value to what sklearn provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_tree = pred_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741694912691\n",
      "0.741694912691\n",
      "Since the above match, we can see that we have correctly calculated the r2 value.\n"
     ]
    }
   ],
   "source": [
    "def r2(actual, preds):\n",
    "    '''\n",
    "    INPUT:\n",
    "    actual - numpy array or pd series of actual y values\n",
    "    preds - numpy array or pd series of predicted y values\n",
    "    OUTPUT:\n",
    "    returns the r-squared score as a float\n",
    "    '''\n",
    "    sse = np.sum((actual-preds)**2)\n",
    "    sst = np.sum((actual-np.mean(actual))**2)\n",
    "    return 1 - sse/sst\n",
    "\n",
    "# Check solution matches sklearn\n",
    "print(r2(y_test, preds_tree))\n",
    "print(r2_score(y_test, preds_tree))\n",
    "print(\"Since the above match, we can see that we have correctly calculated the r2 value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.5481437126\n",
      "19.5481437126\n",
      "If the above match, you are all set!\n"
     ]
    }
   ],
   "source": [
    "def mse(actual, preds):\n",
    "    '''\n",
    "    INPUT:\n",
    "    actual - numpy array or pd series of actual y values\n",
    "    preds - numpy array or pd series of predicted y values\n",
    "    OUTPUT:\n",
    "    returns the mean squared error as a float\n",
    "    '''\n",
    "    \n",
    "    return np.sum((actual-preds)**2)/len(actual)\n",
    "\n",
    "\n",
    "# Check your solution matches sklearn\n",
    "print(mse(y_test, preds_tree))\n",
    "print(mean_squared_error(y_test, preds_tree))\n",
    "print(\"If the above match, you are all set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 9:** Now one last time - complete the function related to mean absolute error.  Then check your function against the sklearn metric to assure they match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12694610778\n",
      "3.12694610778\n",
      "If the above match, you are all set!\n"
     ]
    }
   ],
   "source": [
    "def mae(actual, preds):\n",
    "    '''\n",
    "    INPUT:\n",
    "    actual - numpy array or pd series of actual y values\n",
    "    preds - numpy array or pd series of predicted y values\n",
    "    OUTPUT:\n",
    "    returns the mean absolute error as a float\n",
    "    '''\n",
    "    \n",
    "    return np.sum(abs(actual-preds))/len(actual)\n",
    "\n",
    "# Check your solution matches sklearn\n",
    "print(mae(y_test, preds_tree))\n",
    "print(mean_absolute_error(y_test, preds_tree))\n",
    "print(\"If the above match, you are all set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 10:** Which model performed the best in terms of each of the metrics?  Note that r2 and mse will always match, but the mae may give a different best model.  Use the dictionary and space below to match the best model via each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  The random forest was best in terms of all the metrics this time!\n"
     ]
    }
   ],
   "source": [
    "#match each metric to the model that performed best on it\n",
    "a = 'decision tree'\n",
    "b = 'random forest'\n",
    "c = 'adaptive boosting'\n",
    "d = 'linear regression'\n",
    "\n",
    "\n",
    "best_fit = {\n",
    "    'mse': b,\n",
    "    'r2': b,\n",
    "    'mae': b\n",
    "}\n",
    "\n",
    "#Tests your answer - don't change this code\n",
    "t.check_ten(best_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decistion Tree Mean Squared Error:  19.54814371257485\n",
      "Decistion Tree Mean Absolute Error:  3.126946107784431\n",
      "Decistion Tree R2:  0.7416949126909023\n",
      "\n",
      "\n",
      "Random Forest Mean Squared Error:  11.296195209580837\n",
      "Random Forest Mean Absolute Error:  2.332215568862276\n",
      "Random Forest R2:  0.8507344363345151\n",
      "\n",
      "\n",
      "AdaBoost Mean Squared Error:  14.89330602248744\n",
      "AdaBoost Mean Absolute Error:  2.6956113442133\n",
      "AdaBoost R2:  0.80320296550792\n",
      "\n",
      "\n",
      "Linear Regression Mean Squared Error:  20.74714336030895\n",
      "Linear Regression Mean Absolute Error:  3.1512878365883963\n",
      "Linear Regression R2:  0.7258515818230048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cells for work\n",
    "def printPerformance(modelname, preds, actual = y_test):\n",
    "    print('')\n",
    "    print(modelname, 'Mean Squared Error: ', format(mean_squared_error(actual, preds)))\n",
    "    print(modelname, 'Mean Absolute Error: ', format(mean_absolute_error(actual, preds)))\n",
    "    print(modelname, 'R2: ', format(r2_score(actual, preds)))\n",
    "    print('')\n",
    "\n",
    "\n",
    "printPerformance(modelname='Decistion Tree', preds=pred_dt)\n",
    "printPerformance(modelname='Random Forest', preds=pred_rf)\n",
    "printPerformance(modelname='AdaBoost', preds=pred_ab)\n",
    "printPerformance(modelname='Linear Regression', preds=pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
