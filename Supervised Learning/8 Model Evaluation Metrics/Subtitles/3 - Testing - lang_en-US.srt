1
00:00:00,090 --> 00:00:02,290
In this lesson,
we'll use a concepts of regression and

2
00:00:02,290 --> 00:00:06,400
classification repeatedly, so
let's first recall their definitions.

3
00:00:06,400 --> 00:00:11,336
A regression model is one that predicts
a value like 4 minus 3, or 6.7.

4
00:00:11,336 --> 00:00:15,910
In the graph on the left, we have gone
a line that fits the data closely.

5
00:00:15,910 --> 00:00:17,940
If we have a new value on the x axis,

6
00:00:17,940 --> 00:00:21,750
we approximate by finding
the corresponding y value over the line.

7
00:00:21,750 --> 00:00:24,980
A classification problem is when
one wants to determine a state,

8
00:00:24,980 --> 00:00:29,210
such as positive, negative, or
such as yes, no, or cat, dog.

9
00:00:29,210 --> 00:00:34,020
In the graph in the right, we have
some blue points labelled positive and

10
00:00:34,020 --> 00:00:35,940
some red points labelled negative.

11
00:00:35,939 --> 00:00:38,359
And we have drawn a line
that separates them.

12
00:00:38,359 --> 00:00:40,269
If we have a new value in the plane,

13
00:00:40,270 --> 00:00:44,340
we guess its state based on which
of the two regions it belongs.

14
00:00:44,340 --> 00:00:47,600
So to summarize,
regression returns a numeric value and

15
00:00:47,600 --> 00:00:49,740
classification returns a state.

16
00:00:49,740 --> 00:00:53,310
So, now that you've built a model,
how do you convince yourself and

17
00:00:53,310 --> 00:00:55,500
others that your model is good?

18
00:00:55,500 --> 00:00:56,969
You do this by testing.

19
00:00:56,969 --> 00:01:00,250
So letâ€™s look at the picture of BUFF
which shows a small regression example,

20
00:01:00,250 --> 00:01:02,640
our data corresponds
to the grade points.

21
00:01:02,640 --> 00:01:05,219
We have trained two
models to fit the data.

22
00:01:05,219 --> 00:01:07,439
One is a line and
the other one is a curve.

23
00:01:08,549 --> 00:01:11,920
Now the question is,
which of these two models is better?

24
00:01:11,920 --> 00:01:14,120
Well the one on the right
fits the data perfectly,

25
00:01:14,120 --> 00:01:16,042
while the one on the left doesn't.

26
00:01:16,042 --> 00:01:18,010
So we're tempted to say
the one on the right.

27
00:01:18,010 --> 00:01:22,350
To see how good the fit is we can
take a new point, this red point.

28
00:01:22,349 --> 00:01:26,164
The point seems to be well approximated
by the model in the left, but

29
00:01:26,165 --> 00:01:29,410
is very badly approximated
by the model in the right.

30
00:01:29,409 --> 00:01:32,500
So it seems that maybe the model in
the left is better than the model in

31
00:01:32,500 --> 00:01:33,680
the right, after all.

32
00:01:33,680 --> 00:01:36,630
What makes the model in the left better,
is that even though it doesn't

33
00:01:36,629 --> 00:01:40,847
fit the data perfectly, it generalizes
better than the one on the right.

34
00:01:40,847 --> 00:01:43,480
The model in the right tries
to fit the data too well, and

35
00:01:43,480 --> 00:01:45,020
it ends up memorizing it.

36
00:01:45,019 --> 00:01:48,839
This is called overfitting, and we
will learn it later in the nanodegree.

37
00:01:48,840 --> 00:01:53,109
Now the question is, how do we find
a model that generalizes well?

38
00:01:53,109 --> 00:01:56,250
This is where we introduce
the concept of testing.

39
00:01:56,250 --> 00:02:01,409
What we'll do here is we'll split our
data into two sets, the training set and

40
00:02:01,409 --> 00:02:03,099
the testing set.

41
00:02:03,099 --> 00:02:07,449
In this figure the training set
is the set of grey points, and

42
00:02:07,450 --> 00:02:09,740
the testing set is
the set of white points.

43
00:02:09,740 --> 00:02:13,680
And now what we'll do is as the name
suggests, we'll use a training set to

44
00:02:13,680 --> 00:02:17,875
train the model, and then we test
our results in the testing set.

45
00:02:17,875 --> 00:02:21,495
So, here we have two copies of our sets
to the training set formed by the gray

46
00:02:21,495 --> 00:02:24,314
points and the testing set
formed by the white points.

47
00:02:24,314 --> 00:02:27,974
We can see two models that we train
on the training set, the gray points.

48
00:02:27,974 --> 00:02:31,905
Here the model on the right seems to
be better than the model on the left.

49
00:02:31,905 --> 00:02:36,396
But once we test on the testing set,
the white points, we see that the model

50
00:02:36,395 --> 00:02:40,639
on the left is much better,
as the errors in red are much smaller.

51
00:02:40,639 --> 00:02:43,189
Thus we conclude that the model
on the left is better.

52
00:02:43,189 --> 00:02:46,609
Because although it performs a little
less well on the training set,

53
00:02:46,610 --> 00:02:48,885
it does much better on the testing set.

54
00:02:48,884 --> 00:02:51,894
We can do the same procedure on
a classification problem such as the one

55
00:02:51,895 --> 00:02:52,835
here.

56
00:02:52,835 --> 00:02:56,795
We have trained two classification
models to separate the blue positive and

57
00:02:56,794 --> 00:02:58,634
the red negative points.

58
00:02:58,634 --> 00:03:01,974
This one on the left is okay,
as it only makes a few mistakes.

59
00:03:01,974 --> 00:03:03,814
This one on the right does great,

60
00:03:03,814 --> 00:03:06,344
because it manages to
separate all the points.

61
00:03:06,344 --> 00:03:09,740
But our intuition tells us that
the one left may be better,

62
00:03:09,740 --> 00:03:11,320
as it is more general.

63
00:03:11,319 --> 00:03:15,150
Meanwhile, the model in the left
treats these outliers as noise, and

64
00:03:15,150 --> 00:03:19,099
it tries to fit the data in
a simpler and more general way.

65
00:03:19,099 --> 00:03:21,710
So in order to pick a good model,
we'll take some points and

66
00:03:21,710 --> 00:03:23,510
call them the testing set.

67
00:03:23,509 --> 00:03:26,919
The training set is then given by
the points that are filled in,

68
00:03:26,919 --> 00:03:30,239
while the testing set is given by
the points that are not filled in.

69
00:03:30,240 --> 00:03:32,010
Now we train the two models.

70
00:03:32,009 --> 00:03:35,310
Notice that the two models
fit the training set well.

71
00:03:35,310 --> 00:03:39,030
But once we introduce the testing set,
the model in the left

72
00:03:39,030 --> 00:03:43,879
only makes one mistake, whereas the
model in the right makes two mistakes.

73
00:03:43,879 --> 00:03:48,469
Thus by testing we conclude that
the model in the left is better.

74
00:03:48,469 --> 00:03:51,046
The way to do this in
scikit-learn is very simple using

75
00:03:51,046 --> 00:03:54,359
the function train-test-split
from the model selection package.

76
00:03:54,360 --> 00:03:56,816
First, we import train_test_split.

77
00:03:56,816 --> 00:04:00,195
The train_test_split function
takes the following as arguments.

78
00:04:00,195 --> 00:04:04,280
The input, the output, and

79
00:04:04,280 --> 00:04:07,469
the percentage of data we want
to leave as testing data.

80
00:04:07,469 --> 00:04:11,800
For example, test size equals
0.25 means that 25 percent of our

81
00:04:11,800 --> 00:04:14,400
data will be used as testing.

82
00:04:14,400 --> 00:04:16,959
In this example, we have 16 data points.

83
00:04:16,959 --> 00:04:19,420
So 4 of them will be
used as testing data, and

84
00:04:19,420 --> 00:04:22,150
the remaining 12 as training data.

85
00:04:22,149 --> 00:04:25,459
Now there is a golden rule that
we must never, ever break.

86
00:04:25,459 --> 00:04:30,579
The rule says, thou shalt never use
your testing data for training.

87
00:04:30,579 --> 00:04:33,000
This is very important when
we select some data for

88
00:04:33,000 --> 00:04:37,771
testing, we must put it aside, and
not look at it until the very end.

89
00:04:37,771 --> 00:04:41,379
And we should not use it
to train the algorithm.

90
00:04:41,379 --> 00:04:44,719
This rule is actually very easy
to accidentally break, and

91
00:04:44,720 --> 00:04:45,640
you'll see why very soon.

