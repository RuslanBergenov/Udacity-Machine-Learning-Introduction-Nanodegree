1
00:00:00,000 --> 00:00:04,852
首先观察两个方程式得到相同的线段

2
00:00:04,852 --> 00:00:07,500
线段的方程式是 X1+X2=0

3
00:00:07,500 --> 00:00:10,664
原因在于第二个解法

4
00:00:10,664 --> 00:00:15,839
实际上只是第一个解法的纯量倍数 那么我们来看一下

5
00:00:15,839 --> 00:00:18,929
回想一下 预测是一个 sigmoid 线性函数

6
00:00:18,929 --> 00:00:20,579
所以在第一个例子中 对于点 (1,1)

7
00:00:20,579 --> 00:00:22,788
可以得到 σ (1+1)

8
00:00:22,789 --> 00:00:27,600
即 σ 2 等于 0.88

9
00:00:27,600 --> 00:00:29,370
因为这个点是蓝色的 所以结果不算差

10
00:00:29,370 --> 00:00:31,268
所以标签为 1

11
00:00:31,268 --> 00:00:32,460
对于点 (-1,-1)

12
00:00:32,460 --> 00:00:35,880
预测是 σ (-1-1)

13
00:00:35,880 --> 00:00:40,304
即 σ -2 等于 0.12

14
00:00:40,304 --> 00:00:45,359
因为这个点标签为 0 是红色的 所以结果不是最佳

15
00:00:45,359 --> 00:00:48,009
我们来看一下第二个模型

16
00:00:48,009 --> 00:00:51,335
点 (1,1) 得到预测是

17
00:00:51,335 --> 00:00:55,469
σ (10*1+10*1) 也就是 σ 20

18
00:00:55,469 --> 00:01:01,920
结果是 0.9999999979

19
00:01:01,920 --> 00:01:04,109
非常接近 1

20
00:01:04,109 --> 00:01:06,015
所以这是个很好的预测

21
00:01:06,015 --> 00:01:08,400
对于点 (-1,-1)

22
00:01:08,400 --> 00:01:13,350
σ (10*-1+10*-1)

23
00:01:13,349 --> 00:01:16,837
也就是 σ (-20)

24
00:01:16,837 --> 00:01:23,409
等于 0.0000000021

25
00:01:23,409 --> 00:01:27,256
非常接近 0 所以这是个很好的预测

26
00:01:27,256 --> 00:01:30,143
所以这个测试的答案是第二个模型

27
00:01:30,143 --> 00:01:32,150
第二个模型非常准确

28
00:01:32,150 --> 00:01:33,765
也意味着更好 对不对？

29
00:01:33,765 --> 00:01:35,909
最后一部分后 你觉得有点勉强

30
00:01:35,909 --> 00:01:38,909
因为这可能有点过度拟合

31
00:01:38,909 --> 00:01:40,954
你的预感是正确的

32
00:01:40,954 --> 00:01:43,814
这个问题过度拟合 不过程度比较轻微

33
00:01:43,814 --> 00:01:46,259
这是整个过程 以及第一个模型更好的原因

34
00:01:46,260 --> 00:01:49,206
即使得出更大的误差

35
00:01:49,206 --> 00:01:54,150
我们把 sigmoid 运用到较小值中 如 X1+X2

36
00:01:54,150 --> 00:01:59,484
我们得到左侧的函数 可以有梯度下降更好的坡度

37
00:01:59,484 --> 00:02:07,594
我们把线性函数乘以 10 后得到 σ (10X1+10X2)

38
00:02:07,594 --> 00:02:11,919
由于它们更接近 0 和 1 所以预测更好一些

39
00:02:11,919 --> 00:02:17,737
但是函数更加陡峭 这里也更难进行较大幅度的下降

40
00:02:17,737 --> 00:02:19,960
因为导数非常接近 0

41
00:02:19,960 --> 00:02:24,960
到达曲线中部时 导数非常大

42
00:02:24,960 --> 00:02:27,939
所以 为了合理使用梯度下降法

43
00:02:27,939 --> 00:02:33,585
相比右侧的模型来说 我们更想用左侧的模型

44
00:02:33,585 --> 00:02:35,200
从概念上来说

45
00:02:35,199 --> 00:02:37,259
右侧的模型非常稳定

46
00:02:37,259 --> 00:02:41,454
很难运用梯度下降法

47
00:02:41,455 --> 00:02:42,550
同时正如我们想象的

48
00:02:42,550 --> 00:02:45,910
这些点在右侧模型中分类错误

49
00:02:45,909 --> 00:02:51,250
会产生更大的误差 很难调优模型或纠正模型

50
00:02:51,250 --> 00:02:53,379
这可以引用著名哲学家和数学家

51
00:02:53,379 --> 00:02:58,264
伯特兰·罗素 (Bertrand Russell) 的名言进行总结

52
00:02:58,264 --> 00:03:00,819
人工智能的整个问题

53
00:03:00,819 --> 00:03:04,194
在于错误模型对自身非常确定

54
00:03:04,194 --> 00:03:07,634
而好的模型充满疑问

55
00:03:07,634 --> 00:03:08,764
现在问题是

56
00:03:08,764 --> 00:03:12,422
我们如何避免这种过度拟合发生呢？

57
00:03:12,423 --> 00:03:16,775
由于错误模型提供了更小误差 所以这并不简单

58
00:03:16,775 --> 00:03:20,865
我们要做的是对误差函数稍作调整

59
00:03:20,865 --> 00:03:24,250
大体来说 我们想要惩罚高系数

60
00:03:24,250 --> 00:03:25,969
所以我们这里要做的是

61
00:03:25,969 --> 00:03:32,164
利用原来的误差函数 权重大时 添加较大的一项

62
00:03:32,164 --> 00:03:34,569
两种方法可以做到这一点

63
00:03:34,569 --> 00:03:40,935
第一种方法是加上权重乘以常量 λ 绝对值的总和

64
00:03:40,935 --> 00:03:46,490
另一种方法是加上权重平方总和 再乘以相同的常数

65
00:03:46,490 --> 00:03:52,082
可以看到 如果权重大 这两个函数都大

66
00:03:52,082 --> 00:03:56,430
参数 λ 告诉我们惩罚系数的多少

67
00:03:56,430 --> 00:03:57,467
如果 λ 很大

68
00:03:57,467 --> 00:03:59,085
惩罚会很严重

69
00:03:59,085 --> 00:04:02,875
如果 λ 很小 那么无需对其进行严重的惩罚

70
00:04:02,875 --> 00:04:05,775
最后如果我们想要得到绝对值

71
00:04:05,775 --> 00:04:09,138
可以使用 L1 正则化

72
00:04:09,138 --> 00:04:10,944
如果想要得到平方

73
00:04:10,944 --> 00:04:13,969
可以使用 L2 正则化

74
00:04:13,969 --> 00:04:15,294
两者都比较通用

75
00:04:15,294 --> 00:04:18,069
这取决于我们的目标

76
00:04:18,069 --> 00:04:20,724
我们可以两者选其一

77
00:04:20,725 --> 00:04:27,185
对于使用 L1 和 L2 正则化 这里有一些通用规则

78
00:04:27,185 --> 00:04:32,634
使用 L1 正则化时 我们希望得到稀疏向量

79
00:04:32,634 --> 00:04:36,449
它表示较小权重趋向于 0

80
00:04:36,449 --> 00:04:40,449
所以如果我们想降低权重值 最终得到较小的数

81
00:04:40,449 --> 00:04:41,944
我们可以使用 L1

82
00:04:41,944 --> 00:04:44,550
这也利于特征选择

83
00:04:44,550 --> 00:04:47,550
有时候我们会遇到几百个特征问题

84
00:04:47,550 --> 00:04:52,379
并且L1 正则化可以帮我们选择哪一些更重要

85
00:04:52,379 --> 00:04:55,129
然后将其余的变为 0

86
00:04:55,129 --> 00:04:56,939
而 L2 正则化

87
00:04:56,939 --> 00:04:59,519
不支持稀疏向量

88
00:04:59,519 --> 00:05:02,894
因为它确保所有权重一致较小

89
00:05:02,894 --> 00:05:06,329
这个一般可以训练模型 得出更好结果

90
00:05:06,329 --> 00:05:09,854
所以这会是我们最常用的 现在我们来思考一下

91
00:05:09,855 --> 00:05:13,710
为什么 L1 正则化得出稀疏权重的向量

92
00:05:13,709 --> 00:05:18,495
而 L1 正则化得出较小同质权重的向量？

93
00:05:18,495 --> 00:05:20,545
原因是这样的

94
00:05:20,545 --> 00:05:23,030
如果我们利用向量 (1,0)

95
00:05:23,029 --> 00:05:26,384
权重绝对值的总和是 1

96
00:05:26,384 --> 00:05:30,267
权重平方的总和也是 1

97
00:05:30,267 --> 00:05:35,187
不过如果得到向量 (0.5, 0.5)

98
00:05:35,187 --> 00:05:39,298
权重绝对值的总和仍然是 1

99
00:05:39,298 --> 00:05:46,024
平方总和是 0.25+0.25 等于 0.5

100
00:05:46,024 --> 00:05:51,245
那么 L2 正则化更倾向于向量 (0.5, 0.5)

101
00:05:51,245 --> 00:05:53,599
而不是向量 (1,0)

102
00:05:53,600 --> 00:05:57,020
因为前者可以得出更小的平方总和

103
00:05:57,019 --> 00:06:00,000
反过来说得到更小的函数

