1
00:00:00,300 --> 00:00:04,300
Now let's look carefully at the graph
we have obtained underneath.

2
00:00:04,300 --> 00:00:08,080
This is what's called
a model complexity graph.

3
00:00:08,080 --> 00:00:11,710
Notice that on the left we have
a model that underfits, and

4
00:00:11,710 --> 00:00:14,620
gives us high training and
testing errors.

5
00:00:14,620 --> 00:00:16,930
The model on the right overfits, and

6
00:00:16,930 --> 00:00:21,330
it gives us a very low training error,
but a high testing error.

7
00:00:21,330 --> 00:00:24,330
The model in the middle is just right,
and

8
00:00:24,330 --> 00:00:27,830
it gives us relatively low training and
testing errors.

9
00:00:27,830 --> 00:00:29,730
This is the model we should pick.

10
00:00:29,730 --> 00:00:31,630
So here we have a bit
of a clearer picture.

11
00:00:31,630 --> 00:00:34,530
In the x axis we have
the complexity of the model.

12
00:00:35,800 --> 00:00:41,700
So we go from linear, degree 1,
all the way to polynomial, degree 4.

13
00:00:41,700 --> 00:00:45,750
On the left the models show
underfitting or high bias error.

14
00:00:45,760 --> 00:00:50,500
On the right We see overfitting,
or high variance error.

15
00:00:50,500 --> 00:00:53,870
And in the middle, around
the value 2 there is a happy point

16
00:00:53,870 --> 00:00:57,390
where we are neither underfitting nor
overfitting.

17
00:00:57,390 --> 00:01:03,750
Thus, we decided the best model for
our data is a polynomial of degree 2.

18
00:01:03,750 --> 00:01:05,330
So far so good, right?

19
00:01:05,330 --> 00:01:06,700
Whats happening?

20
00:01:06,700 --> 00:01:06,710
What is this?
Whats happening?

21
00:01:06,710 --> 00:01:07,270
What is this?

22
00:01:08,450 --> 00:01:09,720
No, what do we do?

23
00:01:09,720 --> 00:01:11,190
We broke the golden rule.

24
00:01:11,190 --> 00:01:14,890
You should never use your
testing data for training.

25
00:01:14,890 --> 00:01:17,030
We made a huge mistake.

26
00:01:17,030 --> 00:01:19,470
But let's see, what did we do here?

27
00:01:19,470 --> 00:01:21,310
Here is the problem.

28
00:01:21,310 --> 00:01:24,760
We used our testing data
to train our model.

29
00:01:24,760 --> 00:01:29,120
We're not supposed to use our
testing data until the very end.

30
00:01:29,120 --> 00:01:32,080
And we used it to make a huge
decision about our model.

31
00:01:32,080 --> 00:01:33,640
This is completely forbidden.

32
00:01:33,640 --> 00:01:36,200
You're not allowed to
touch your testing data.

33
00:01:36,200 --> 00:01:37,910
So how do we fix this?

34
00:01:37,910 --> 00:01:41,420
How do we make a good decision about our
model without using the testing data?

35
00:01:42,550 --> 00:01:43,810
Well, no need to panic.

36
00:01:43,810 --> 00:01:47,000
We can fix this by breaking
our data set even more.

37
00:01:47,000 --> 00:01:48,800
Now, instead of having a training and

38
00:01:48,810 --> 00:01:53,090
a testing set,
we'll add a cross validation set.

39
00:01:53,090 --> 00:01:56,990
Now, the training set will be used for
training the parameters.

40
00:01:56,990 --> 00:01:59,170
The cross validation
set will be used for

41
00:01:59,180 --> 00:02:03,780
making decisions about the model,
such as the degree of the polynomial.

42
00:02:03,780 --> 00:02:08,030
And the testing set will be used for
the final testing of the model.

43
00:02:08,030 --> 00:02:09,030
Now our graph looks so

44
00:02:09,030 --> 00:02:12,900
much nicer with a cross validation
error instead of a testing error.

45
00:02:12,900 --> 00:02:14,310
So let's recap.

46
00:02:14,310 --> 00:02:18,160
Here we have the model complexity
graph and the example all together.

47
00:02:18,160 --> 00:02:18,170
On the left, we see underfitting or
an oversimplification of the problem.
Here we have the model complexity
graph and the example all together.

48
00:02:18,170 --> 00:02:23,410
On the left, we see underfitting or
an oversimplification of the problem.

49
00:02:23,410 --> 00:02:27,780
This is bad on both the training and
the cross validation set

50
00:02:27,780 --> 00:02:31,980
because our model simply does not
capture the complexity of our data.

51
00:02:31,990 --> 00:02:36,910
On the right, we see overfitting or
an over complication of the problem.

52
00:02:36,910 --> 00:02:36,920
This is great on the training set,
because we are memorizing it, but
On the right, we see overfitting or
an over complication of the problem.

53
00:02:36,920 --> 00:02:40,700
This is great on the training set,
because we are memorizing it, but

54
00:02:40,710 --> 00:02:44,390
bad on the cross validation set because
the model does not generalize well.

55
00:02:45,690 --> 00:02:49,540
And in the middle we have the perfect
model which is good in both the training

56
00:02:49,540 --> 00:02:51,160
and the testing set.

57
00:02:51,160 --> 00:02:54,710
I like to imagine underfitting
as coming to an exam unprepared.

58
00:02:54,710 --> 00:02:57,530
No matter how hard you
try you won't do well.

59
00:02:57,530 --> 00:03:00,390
Overfitting is like studying for
an exam but instead of

60
00:03:00,390 --> 00:03:05,300
trying to understand the material,
you just memorize the book word by word.

61
00:03:05,300 --> 00:03:07,120
You can repeat anything that's given,
but

62
00:03:07,120 --> 00:03:10,310
you won't be able to answer
new questions about the data.

63
00:03:10,310 --> 00:03:13,080
A good model is like studying for
an exam and

64
00:03:13,080 --> 00:03:15,220
coming ready to answer any
questions in the material.

65
00:03:16,530 --> 00:03:19,500
And here is just a general picture
of the model complexity graph.

66
00:03:19,500 --> 00:03:22,370
Of course they may not look as pretty,
but in real life you see that most of

67
00:03:22,370 --> 00:03:25,000
the time your models will exhibit
a behavior like this one.

68
00:03:25,000 --> 00:03:29,560
Wheres the model gets more complex, the
training error gets smaller and smaller.

69
00:03:29,560 --> 00:03:33,930
And the testing error starts big,
then decreases and then increases again.

70
00:03:33,930 --> 00:03:36,440
On the left you underfit.

71
00:03:36,440 --> 00:03:38,790
On the right, you overfit.

72
00:03:38,790 --> 00:03:38,800
And the perfect point is here,
On the right, you overfit.

73
00:03:38,800 --> 00:03:40,420
And the perfect point is here,

74
00:03:40,430 --> 00:03:42,450
where the graphs just start
to distance from each other.

