1
00:00:00,000 --> 00:00:03,524
Correct. The first way to split them does not help us at all.

2
00:00:03,524 --> 00:00:05,370
We ended up with very similar sets of

3
00:00:05,370 --> 00:00:08,085
red and blue points and we learn nothing about the data.

4
00:00:08,085 --> 00:00:09,915
The second way did a decent job.

5
00:00:09,914 --> 00:00:11,819
We managed to get most of the blue ones on

6
00:00:11,820 --> 00:00:14,669
one side and most of the red ones on the other side.

7
00:00:14,669 --> 00:00:16,980
So, now we know a bit more about it.

8
00:00:16,980 --> 00:00:18,690
And the third one did fantastic.

9
00:00:18,690 --> 00:00:20,490
It managed to send all the blue points on

10
00:00:20,489 --> 00:00:23,039
one side and all the red ones on the other side.

11
00:00:23,039 --> 00:00:24,914
Now we know a lot about our data.

12
00:00:24,914 --> 00:00:28,750
Actually, we'll learn how to calculate something called the information gain,

13
00:00:28,750 --> 00:00:31,300
and it will be zero for the first splitting,

14
00:00:31,300 --> 00:00:34,299
0.28 for the second, and one for the third one.

15
00:00:34,299 --> 00:00:36,820
And the formula for information gain is very simple.

16
00:00:36,820 --> 00:00:38,500
It's just the change in entropy.

17
00:00:38,500 --> 00:00:40,203
Let me be more specific.

18
00:00:40,203 --> 00:00:42,190
For every node in the Decision Tree,

19
00:00:42,189 --> 00:00:45,714
we can calculate the entropy of the data in the parent node.

20
00:00:45,715 --> 00:00:49,630
And then, we calculate the entropies of the two children.

21
00:00:49,630 --> 00:00:52,630
The information gain is a difference between

22
00:00:52,630 --> 00:00:56,890
the entropy of the parent and the average entropy of the children.

23
00:00:56,890 --> 00:00:58,420
So, in our first example,

24
00:00:58,420 --> 00:01:01,454
you can calculate the entropy of the parent and its one.

25
00:01:01,454 --> 00:01:04,504
The entropy of each of the children is 0.72.

26
00:01:04,504 --> 00:01:08,342
So the average of the entropy of the children is actually 0.72.

27
00:01:08,343 --> 00:01:14,060
Thus, the change in entropy is one minus 0.72, which is 0.28.

28
00:01:14,060 --> 00:01:18,125
For this example, now, we can calculate the entropies of the children which are both one.

29
00:01:18,125 --> 00:01:22,099
Thus, the change in entropy is one minus one which is zero.

30
00:01:22,099 --> 00:01:25,399
This is a terrible splitting because they gave us zero information.

31
00:01:25,400 --> 00:01:26,870
And finally, in this example,

32
00:01:26,870 --> 00:01:29,000
the entropy of the children are both zero,

33
00:01:29,000 --> 00:01:30,349
since as we saw,

34
00:01:30,349 --> 00:01:33,664
a set where the points are the same color has zero entropy.

35
00:01:33,665 --> 00:01:37,315
Thus, the information gain is one minus zero which is one.

36
00:01:37,314 --> 00:01:41,174
This split gave us the largest information gain and as you can see,

37
00:01:41,174 --> 00:01:43,784
it is the best split because it managed to

38
00:01:43,784 --> 00:01:47,054
perfectly cut the data into the blue points and red points.

39
00:01:47,055 --> 00:01:51,116
So, here's our summary. The three cuts with the values of information gain.

40
00:01:51,115 --> 00:01:52,694
If the tree had to choose,

41
00:01:52,694 --> 00:01:54,389
it would go for the third cut,

42
00:01:54,390 --> 00:01:58,200
which is the one that gives it the largest amount of information gain.

