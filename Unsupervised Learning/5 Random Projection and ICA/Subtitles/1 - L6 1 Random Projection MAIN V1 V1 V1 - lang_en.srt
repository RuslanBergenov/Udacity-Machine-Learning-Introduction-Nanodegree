1
00:00:00,000 --> 00:00:02,555
Hello, I'm J. In this lesson,

2
00:00:02,555 --> 00:00:05,175
we'll be talking about dimensionality reduction.

3
00:00:05,174 --> 00:00:08,910
The first method we'll look at in this lesson is random projection,

4
00:00:08,910 --> 00:00:11,460
which is a powerful dimensionality reduction method

5
00:00:11,460 --> 00:00:14,255
that is computationally more efficient than PCA.

6
00:00:14,255 --> 00:00:17,010
It is commonly used in cases where a dataset has

7
00:00:17,010 --> 00:00:20,405
too many dimensions for PCA to be directly computed.

8
00:00:20,405 --> 00:00:25,250
Let's say your application is running on a system with limited computational resources,

9
00:00:25,250 --> 00:00:29,329
or you just find that PCA is too taxing for a specific situation that you're in.

10
00:00:29,329 --> 00:00:31,289
Just like PCA, it takes a dataset.

11
00:00:31,289 --> 00:00:34,685
Let's say this is our dataset with d dimensions,

12
00:00:34,685 --> 00:00:38,469
let's say 1,000 and a certain number of samples or rows,

13
00:00:38,469 --> 00:00:40,320
let's say n. These are columns.

14
00:00:40,320 --> 00:00:42,905
So, it takes our dataset and it produces

15
00:00:42,905 --> 00:00:47,564
a transformation of it that is in a much smaller number of columns.

16
00:00:47,564 --> 00:00:50,164
So, okay, let's say 50 for example,

17
00:00:50,164 --> 00:00:52,899
but the same number of samples and where your each column

18
00:00:52,899 --> 00:00:55,869
here it captures information from multiple columns there.

19
00:00:55,869 --> 00:00:59,119
Let's look at an oversimplified example for reducing

20
00:00:59,119 --> 00:01:02,989
the dimensions of a dataset from two dimensions into one dimension.

21
00:01:02,990 --> 00:01:05,379
PCA here will try to maximize variance.

22
00:01:05,379 --> 00:01:09,679
So, it finds the vector or direction that maximizes the variance so it

23
00:01:09,680 --> 00:01:11,570
loses the least amount of information when it

24
00:01:11,569 --> 00:01:14,539
projects the data from two dimensions to one.

25
00:01:14,540 --> 00:01:16,430
So, that line would be something like this,

26
00:01:16,430 --> 00:01:18,170
and these would be the projections,

27
00:01:18,170 --> 00:01:19,980
and so in one dimension,

28
00:01:19,980 --> 00:01:21,770
the dataset would look like this.

29
00:01:21,769 --> 00:01:23,989
Random projection, so with this calculation that PCA

30
00:01:23,989 --> 00:01:26,629
did especially if you're talking about a lot of dimensions,

31
00:01:26,629 --> 00:01:28,409
it consumes a certain amount of resources.

32
00:01:28,409 --> 00:01:30,619
Random projection just say, pick a line,

33
00:01:30,620 --> 00:01:34,689
any line, we'll do a projection on that, that's our dataset.

34
00:01:34,689 --> 00:01:37,640
So, while it doesn't really make a lot of sense in

35
00:01:37,640 --> 00:01:40,730
our over-simplified scenario like this from two dimensions to one,

36
00:01:40,730 --> 00:01:44,210
it actually works and it works really well in higher dimensions,

37
00:01:44,209 --> 00:01:47,029
and it works in a high-performance way.

38
00:01:47,030 --> 00:01:50,000
The basic premise of a random projection is that we can

39
00:01:50,000 --> 00:01:52,790
simply reduce the number of dimensions in our dataset,

40
00:01:52,790 --> 00:01:56,725
by multiplying it by a random matrix like this.

41
00:01:56,724 --> 00:02:02,629
So, d, we will have d in our dataset but k is something we either compute or it's

42
00:02:02,629 --> 00:02:05,509
something that we desire and there is a way to compute what is

43
00:02:05,510 --> 00:02:08,640
a conservative or a good estimate for k. So,

44
00:02:08,639 --> 00:02:10,244
that would be the resulting dataset.

45
00:02:10,245 --> 00:02:12,375
Just multiplying by a random matrix,

46
00:02:12,375 --> 00:02:14,884
that's all random projection is in a way.

47
00:02:14,884 --> 00:02:17,254
Let's take a concrete example here,

48
00:02:17,254 --> 00:02:18,840
let's say this is our dataset,

49
00:02:18,840 --> 00:02:21,770
and it has 12,000 dimensions,

50
00:02:21,770 --> 00:02:26,570
that's our d, and it has 1,500 rows or samples.

51
00:02:26,569 --> 00:02:29,870
If we give this to scikit-learn and say, okay, scikit-learn,

52
00:02:29,870 --> 00:02:34,150
can you please do a random projection for this dataset just using your default values?

53
00:02:34,150 --> 00:02:36,950
It will return this dataset for us which it will be in

54
00:02:36,949 --> 00:02:42,174
6,200 dimensions and the same number of samples obviously.

55
00:02:42,175 --> 00:02:46,450
So, how do we know that it works and where does the k come from?

56
00:02:46,449 --> 00:02:50,419
The theoretical underpinning for random projection is this idea called

57
00:02:50,419 --> 00:02:53,799
the Johnson-Lindenstrauss lemma which states

58
00:02:53,800 --> 00:02:57,635
that a dataset of N points in high-dimensional space.

59
00:02:57,634 --> 00:03:00,659
So, this dataset, N points,

60
00:03:00,659 --> 00:03:03,509
high-dimensional space, 12,000 is pretty high.

61
00:03:03,509 --> 00:03:04,954
It can be mapped.

62
00:03:04,955 --> 00:03:06,890
Multiplying by this random matrix,

63
00:03:06,889 --> 00:03:12,789
down to a space in much lower dimension which is this narrow dataset in a way.

64
00:03:12,789 --> 00:03:14,280
This is why it's really important for us.

65
00:03:14,280 --> 00:03:15,770
It can be done in a way that preserves

66
00:03:15,770 --> 00:03:18,855
the distances between the points to a large degree.

67
00:03:18,854 --> 00:03:21,699
So, the distances between each two points,

68
00:03:21,699 --> 00:03:23,629
each pair of points in these datasets,

69
00:03:23,629 --> 00:03:26,990
after projection that is preserved in a certain way.

70
00:03:26,990 --> 00:03:29,469
That's really important because in most or

71
00:03:29,469 --> 00:03:32,050
in a lot of supervised and unsupervised learning,

72
00:03:32,050 --> 00:03:34,765
the algorithms really care about the distances between the points.

73
00:03:34,764 --> 00:03:37,789
So, we have a set a level of guarantee that

74
00:03:37,789 --> 00:03:41,914
these distances will be distorted a little bit but they can be preserved.

75
00:03:41,914 --> 00:03:43,750
How can they be preserved?

76
00:03:43,750 --> 00:03:45,389
What sort of guarantees we have?

77
00:03:45,389 --> 00:03:48,619
Let's take a quick example and just actually calculate that out.

78
00:03:48,620 --> 00:03:52,140
So, let's say we take these first two rows here,

79
00:03:52,139 --> 00:03:53,604
so this one and this one,

80
00:03:53,604 --> 00:03:58,609
the first two points in our dataset and then these are their values after projection.

81
00:03:58,610 --> 00:04:02,210
So, the same samples but the different level of dimensionality.

82
00:04:02,210 --> 00:04:05,430
So, what the Johnson-Lindentrauss lemma tells us that,

83
00:04:05,430 --> 00:04:11,314
the distance between the two points in the projection squared is sort of squeezed.

84
00:04:11,314 --> 00:04:14,810
So, it's larger than the distance between the two points in

85
00:04:14,810 --> 00:04:19,259
the original dataset squared times one minus Epsilon.

86
00:04:19,259 --> 00:04:22,009
So, epsilon is the level of error that we are

87
00:04:22,009 --> 00:04:25,050
allowing the random projection to have in the projection.

88
00:04:25,050 --> 00:04:28,100
So, the distance between the two points in

89
00:04:28,100 --> 00:04:31,910
the projected data set will be larger than one minus Epsilon

90
00:04:31,910 --> 00:04:35,270
times square of distance of the two points in the original dataset

91
00:04:35,269 --> 00:04:39,454
and it will be smaller than one plus Epsilon off that distance squared.

92
00:04:39,454 --> 00:04:41,430
I've actually calculated those numbers.

93
00:04:41,430 --> 00:04:45,805
So, the distance between these two points is 125.6.

94
00:04:45,805 --> 00:04:47,694
So, we put that in here.

95
00:04:47,694 --> 00:04:50,719
Epsilon, the default value so we didn't change anything,

96
00:04:50,720 --> 00:04:53,765
the default value in scikit-learn is 0.1.

97
00:04:53,764 --> 00:04:56,019
It can take any value from 0-1.

98
00:04:56,019 --> 00:04:58,625
So, if we do it like this,

99
00:04:58,625 --> 00:05:01,819
the distance between these two points is 125.8.

100
00:05:01,819 --> 00:05:06,409
So, this distance here will be larger than 0.9

101
00:05:06,410 --> 00:05:11,785
times this distance and smaller than 1.1 times this distance squared.

102
00:05:11,785 --> 00:05:15,235
So, you can see that Epsilon is like a liver.

103
00:05:15,235 --> 00:05:18,650
It goes into the calculation of how many columns are produced,

104
00:05:18,649 --> 00:05:21,319
and it is the level of error that we are allowing

105
00:05:21,319 --> 00:05:25,480
the distortion to have this reduction of dimensionality.

106
00:05:25,480 --> 00:05:30,660
So, this guarantee preserves the distance between every pair of points in the dataset.

107
00:05:30,660 --> 00:05:32,840
So, it's not just one and two,

108
00:05:32,839 --> 00:05:34,909
it's one and every other point,

109
00:05:34,910 --> 00:05:36,090
and two and every other point.

110
00:05:36,089 --> 00:05:38,064
So, that guarantee is there.

111
00:05:38,064 --> 00:05:41,404
An Epsilon is just an input into the function that we can use

112
00:05:41,404 --> 00:05:45,269
to say preserve the distances by this degree.

