1
00:00:00,000 --> 00:00:04,440
首先 我们看看梯度下降算法在干什么

2
00:00:04,440 --> 00:00:08,990
我们位于珠峰山顶上 需要下山

3
00:00:08,990 --> 00:00:10,852
为了下山

4
00:00:10,852 --> 00:00:16,500
我们按照高度（即误差函数）的梯度负值的方向

5
00:00:16,500 --> 00:00:19,280
走了很多步

6
00:00:19,280 --> 00:00:21,289
每步就叫做 epoch

7
00:00:21,289 --> 00:00:23,504
当我们提到步长次数时

8
00:00:23,504 --> 00:00:25,960
我们指的是 epoch 次数

9
00:00:25,960 --> 00:00:27,809
现在我们看看每个 epoch 中发生的情况

10
00:00:27,809 --> 00:00:29,760
在每个 epoch 中 我们获得输入

11
00:00:29,760 --> 00:00:34,920
即所有的数据 让其完成整个神经网络

12
00:00:34,920 --> 00:00:36,840
然后算出预测

13
00:00:36,840 --> 00:00:38,729
我们计算误差

14
00:00:38,728 --> 00:00:41,859
即与实际标签相差多大

15
00:00:41,859 --> 00:00:44,548
最后 我们反向传播此误差

16
00:00:44,548 --> 00:00:48,048
以便更新神经网络中的权重

17
00:00:48,048 --> 00:00:51,405
这样将会提供更好的界线来预测数据

18
00:00:51,405 --> 00:00:53,895
对所有数据完成这一过程

19
00:00:53,895 --> 00:00:55,607
如果有很多很多的数据点

20
00:00:55,606 --> 00:00:57,057
通常都是这样

21
00:00:57,057 --> 00:00:59,660
那么这些就是很大的矩阵计算过程

22
00:00:59,658 --> 00:01:03,992
我会用到大量的内存 而这仅仅是一个步长

23
00:01:03,993 --> 00:01:05,358
如果有很多步长

24
00:01:05,358 --> 00:01:09,058
可以想象要花费多少时间和运算容量

25
00:01:09,060 --> 00:01:12,094
有没有什么方法可以加速这一过程？

26
00:01:12,093 --> 00:01:17,765
有个问题：每次进行一个步长 都需要代入所有数据吗？

27
00:01:17,765 --> 00:01:19,454
如果数据分布很合理

28
00:01:19,453 --> 00:01:21,948
那么一小部分数据就可以很好地告诉我们

29
00:01:21,950 --> 00:01:24,775
梯度是多少

30
00:01:24,775 --> 00:01:28,299
或许不是最准确的梯度估算结果 但是速度很快

31
00:01:28,299 --> 00:01:29,825
因为我们要迭代计算

32
00:01:29,825 --> 00:01:31,599
或许是个好方法

33
00:01:31,599 --> 00:01:35,459
这时候随机梯度下降就派上用场了

34
00:01:35,459 --> 00:01:40,489
随机梯度下降的原理很简单

35
00:01:40,489 --> 00:01:42,424
我们拿出一小部分数据 让它们经历整个神经网络

36
00:01:42,424 --> 00:01:45,319
根据这些点计算误差函数的梯度

37
00:01:45,319 --> 00:01:49,030
然后沿着该方向移动一个步长

38
00:01:49,030 --> 00:01:51,799
现在我们依然要使用所有数据

39
00:01:51,799 --> 00:01:53,390
所以我们的做法是

40
00:01:53,390 --> 00:01:56,564
我们将数据拆分为几个批次

41
00:01:56,563 --> 00:01:59,817
在此示例中 我们有 24 个点

42
00:01:59,819 --> 00:02:03,405
我们将拆分为 4 批 每批 6 个点

43
00:02:03,405 --> 00:02:08,359
拿出第一批次的点并用神经网络处理它们

44
00:02:08,359 --> 00:02:14,000
计算误差和梯度 反向传播以更新权重

45
00:02:14,000 --> 00:02:15,335
这样就得出新的权重

46
00:02:15,335 --> 00:02:19,370
它们将定义出更好的界限区域 从左侧可以看出

47
00:02:19,370 --> 00:02:24,655
现在拿出第二批次的数据 完成相同的流程

48
00:02:24,655 --> 00:02:29,259
得出更好的权重和更好的界限区域

49
00:02:29,258 --> 00:02:32,258
然后对第三批次执行相同的流程

50
00:02:32,258 --> 00:02:37,243
最后对第四批次执行相同的流程 就完成了

51
00:02:37,245 --> 00:02:39,099
注意对于数据

52
00:02:39,098 --> 00:02:41,384
我们执行了 4 个步长

53
00:02:41,383 --> 00:02:43,572
但是对于普通梯度下降

54
00:02:43,574 --> 00:02:46,710
我们仅对所有数据执行了一个步长

55
00:02:46,710 --> 00:02:52,405
当然 所采取的四个步长精确度很低

56
00:02:52,405 --> 00:02:58,566
但在现实中 采取大量稍微不太准确的步长比采取一个很准确的步长要好很多

57
00:02:58,566 --> 00:03:01,062
在此纳米学位课程的稍后阶段

58
00:03:01,062 --> 00:03:04,567
你将有机会应用随机梯度下降 看看它的优势

