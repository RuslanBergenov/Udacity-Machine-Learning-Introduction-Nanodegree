1
00:00:00,000 --> 00:00:02,033
Vimos antes
com o perceptron XOR

2
00:00:02,067 --> 00:00:04,734
que colocar uma 2ª camada
permite que o modelo

3
00:00:04,767 --> 00:00:08,033
ache soluções para problemas
linearmente inseparáveis.

4
00:00:08,067 --> 00:00:11,100
Aqui está um exemplo
de um perceptron multicamadas,

5
00:00:11,133 --> 00:00:14,334
com 3 unidades de entrada,
uma de saída,

6
00:00:14,367 --> 00:00:16,300
e duas unidades no meio.

7
00:00:16,334 --> 00:00:18,868
A camada do meio
chama-se camada oculta.

8
00:00:18,901 --> 00:00:21,734
Calculando a saída desta rede
é como antes,

9
00:00:21,767 --> 00:00:24,968
só que agora, as ativações
da camada oculta são usadas

10
00:00:25,000 --> 00:00:27,200
como a entrada
para a camada de saída.

11
00:00:27,234 --> 00:00:30,200
A entrada para a camada oculta
é a mesma de antes:

12
00:00:30,234 --> 00:00:34,367
Estes pesos vezes os valores de
entrada mais um termo de tendência.

13
00:00:34,400 --> 00:00:37,968
E, como antes, de novo,
você usa uma função de ativação,

14
00:00:38,000 --> 00:00:41,267
como um sigmóide, para calcular
a saída da camada oculta.

15
00:00:41,300 --> 00:00:44,701
As ativações da camada oculta
passam para a camada de saída

16
00:00:44,734 --> 00:00:46,133
com o 2º conjunto de pesos,

17
00:00:46,167 --> 00:00:49,767
e usam uma função de ativação
para obter a saída da rede.

18
00:00:49,801 --> 00:00:51,501
Empilhar mais camadas assim

19
00:00:51,534 --> 00:00:54,367
ajuda a rede de aprender
padrões mais complexos.

20
00:00:54,400 --> 00:00:56,634
Daí vem o nome
da aprendizagem profunda,

21
00:00:56,667 --> 00:00:58,234
e o que a faz tão forte.

22
00:00:58,267 --> 00:01:00,367
Pilhas profundas
de camadas ocultas.

