1
00:00:00,000 --> 00:00:02,294
现在我们来看看决策树中潜在的问题

2
00:00:02,294 --> 00:00:05,549
比如说我们有一张极大的表格 其中行列很多

3
00:00:05,549 --> 00:00:08,580
然后我们建立了决策树  比如说是这个样子的

4
00:00:08,580 --> 00:00:11,339
这只是一个例子 并不是真实的决策树 

5
00:00:11,339 --> 00:00:13,619
然后我们得到了如下结论

6
00:00:13,619 --> 00:00:17,160
如果一位客户为男性 年龄在 15 到 25 之间 居住在美国

7
00:00:17,160 --> 00:00:18,359
使用 Android 还在上学

8
00:00:18,359 --> 00:00:20,925
喜欢网球和披萨 但不喜欢长时间在沙滩上长散步

9
00:00:20,925 --> 00:00:23,255
那么他很可能下载 Pokemon Go

10
00:00:23,254 --> 00:00:27,144
这种做法很不理想 看起来似乎只是在记忆数据

11
00:00:27,144 --> 00:00:31,195
这种情况称为过拟合 决策树经常会过拟合

12
00:00:31,195 --> 00:00:34,420
如果我们选用连续特征也会出现这种问题

13
00:00:34,420 --> 00:00:37,179
决策树有许多结点 它最终会呈现给我们多个几乎与点相接的小方块

14
00:00:37,179 --> 00:00:41,304
这些小方块将不同颜色的点分开

15
00:00:41,304 --> 00:00:45,009
这也是过拟合现象 因为它对数据不具备普适性

16
00:00:45,009 --> 00:00:49,164
那么如何解决这一问题呢  方法很简单

17
00:00:49,164 --> 00:00:51,310
我们可以这样操作

18
00:00:51,310 --> 00:00:53,245
随机从数据中挑选几列

19
00:00:53,244 --> 00:00:55,929
并根据这些列建构决策树

20
00:00:55,929 --> 00:00:58,269
然后随机选取其它几列

21
00:00:58,270 --> 00:01:01,175
再次构建决策树

22
00:01:01,174 --> 00:01:03,754
然后让决策树进行选择

23
00:01:03,755 --> 00:01:05,629
当我们有新的数据时

24
00:01:05,629 --> 00:01:07,414
比如说出现了一个新用户

25
00:01:07,415 --> 00:01:12,620
就只需让所有的决策树做出预测 并选取结果中显示最多的

26
00:01:12,620 --> 00:01:15,410
比如 这些决策树觉得

27
00:01:15,409 --> 00:01:18,935
这个人会下载 Snapchat WhatsApp 和 WhatsApp

28
00:01:18,935 --> 00:01:22,129
因此 决策树的集成会推荐此人下载 WhatsApp

29
00:01:22,129 --> 00:01:25,670
由于我们利用随机的列建构的多个决策树做出了预测

30
00:01:25,670 --> 00:01:28,280
这种方法称为随机森林

31
00:01:28,280 --> 00:01:31,159
其实还有比随机选取列更好的方法

32
00:01:31,159 --> 00:01:34,269
我们将在该纳米课程的集成方法部分进行学习

