1
00:00:00,000 --> 00:00:02,310
In the last video, we'll learned how to decrease

2
00:00:02,310 --> 00:00:05,990
an error function by walking along the negative of its gradient.

3
00:00:05,990 --> 00:00:09,970
Now in this video, we're going to learn formulas for these error functions.

4
00:00:09,970 --> 00:00:14,095
The two most common error functions for linear regression are the mean absolute error,

5
00:00:14,095 --> 00:00:15,885
and the mean squared error.

6
00:00:15,884 --> 00:00:18,015
First, we'll learn the mean absolute error.

7
00:00:18,015 --> 00:00:20,179
So, here's a point and a line,

8
00:00:20,179 --> 00:00:22,710
the point has coordinates x, y,

9
00:00:22,710 --> 00:00:25,370
and the line is called Y-hat since it's the prediction.

10
00:00:25,370 --> 00:00:27,810
So, our prediction for this point is going to be the point

11
00:00:27,809 --> 00:00:30,149
in the line with the same x coordinate as our point,

12
00:00:30,149 --> 00:00:32,479
that is the point x, y hat.

13
00:00:32,479 --> 00:00:37,784
This means the vertical distance from the point to the line is y minus y hat,

14
00:00:37,784 --> 00:00:40,399
and that's what we'll be calling the error.

15
00:00:40,399 --> 00:00:42,619
Notice that this is not the actual distance to

16
00:00:42,619 --> 00:00:44,989
the line since this would be a perpendicular segment,

17
00:00:44,990 --> 00:00:47,060
but it's the vertical distance from the point to

18
00:00:47,060 --> 00:00:50,770
the line which is the distance between the point and its prediction.

19
00:00:50,770 --> 00:00:53,210
Now, our total error is going to be the sum of

20
00:00:53,210 --> 00:00:56,015
all these distances for all the points in our dataset.

21
00:00:56,015 --> 00:00:58,185
And sometimes we'll use this as our error,

22
00:00:58,185 --> 00:01:01,219
but in this case we'll use the average or the mean absolute error,

23
00:01:01,219 --> 00:01:04,319
which is the sum of all the errors divided by m,

24
00:01:04,319 --> 00:01:06,349
the number of points in our dataset.

25
00:01:06,349 --> 00:01:09,329
Using the sum or the average won't change our algorithms,

26
00:01:09,329 --> 00:01:11,530
since that would only scale our error by a constant,

27
00:01:11,530 --> 00:01:13,989
namely m. Notice something here which is

28
00:01:13,989 --> 00:01:16,899
that we have an absolute value around the y minus y hat,

29
00:01:16,900 --> 00:01:19,060
the reason is that if the point is on top of the line,

30
00:01:19,060 --> 00:01:20,680
the distance is y minus y hat,

31
00:01:20,680 --> 00:01:24,175
but if it's under the line then it's y hat minus y.

32
00:01:24,174 --> 00:01:26,579
We want the error to always be positive,

33
00:01:26,579 --> 00:01:29,899
otherwise negative errors will cancel with positive errors.

34
00:01:29,900 --> 00:01:33,685
Therefore, we take the absolute value of y minus y hat.

35
00:01:33,685 --> 00:01:38,890
So, our mean absolute error is the average of all absolute errors or in other words,

36
00:01:38,890 --> 00:01:42,960
the sum of all these absolute values divided by the number of points,

37
00:01:42,959 --> 00:01:47,449
which is m. We're going to plot these error over here in a graph.

38
00:01:47,450 --> 00:01:49,700
Obviously as we mentioned before,

39
00:01:49,700 --> 00:01:54,254
the graph has more dimensions but this is a two-dimensional simplification of that graph.

40
00:01:54,254 --> 00:01:56,959
As we descend in this graph using gradient descent,

41
00:01:56,959 --> 00:01:59,359
we get a better and better line until we find

42
00:01:59,359 --> 00:02:03,120
the best possible fit with the smallest possible mean absolute error.

