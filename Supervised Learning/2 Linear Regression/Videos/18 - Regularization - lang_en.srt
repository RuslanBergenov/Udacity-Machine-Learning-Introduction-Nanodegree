1
00:00:00,000 --> 00:00:04,690
The following concept is one that works both for regression and classification.

2
00:00:04,690 --> 00:00:07,790
So in this video, we'll explain it using a classification problem.

3
00:00:07,790 --> 00:00:12,169
But as you will see, all the arguments here work with regression algorithms as well.

4
00:00:12,169 --> 00:00:14,230
The concept is called regularization,

5
00:00:14,230 --> 00:00:18,115
it's a very useful technique to improve our models and make sure they don't over fit.

6
00:00:18,114 --> 00:00:20,854
So let's look at some data, here's the data,

7
00:00:20,855 --> 00:00:22,550
and let's make two copies of it,

8
00:00:22,550 --> 00:00:25,039
and let's look at two models that classify this data.

9
00:00:25,039 --> 00:00:29,609
The first one is a line and the second one is a higher degree polynomial curve.

10
00:00:29,609 --> 00:00:31,399
So, the question is which one is better?

11
00:00:31,399 --> 00:00:33,869
Well, they both have their pros and cons, right?

12
00:00:33,869 --> 00:00:36,259
The one on the left makes a couple of mistakes.

13
00:00:36,259 --> 00:00:38,169
As you can see, there is a red point and

14
00:00:38,170 --> 00:00:41,335
a blue point in the wrong sides but it is much simpler.

15
00:00:41,335 --> 00:00:44,965
The one on the right makes zero mistakes but it's actually a bit more complicated.

16
00:00:44,965 --> 00:00:47,140
So let's say we want the one on the left because the one on

17
00:00:47,140 --> 00:00:49,780
the right over fits and it just doesn't generalize well.

18
00:00:49,780 --> 00:00:52,579
So the problem is that when we train the model,

19
00:00:52,579 --> 00:00:55,920
the one on the right will appear more likely and the reason is the following.

20
00:00:55,920 --> 00:00:57,255
When we're training the model,

21
00:00:57,255 --> 00:00:59,780
the model takes an error and minimizes it.

22
00:00:59,780 --> 00:01:02,890
So, the model on the left has a small error since it own

23
00:01:02,890 --> 00:01:06,295
misclassifies two points but it's an error nonetheless.

24
00:01:06,295 --> 00:01:08,989
The model in the right has a really small error since it

25
00:01:08,989 --> 00:01:11,449
does not misclassifies any of the points.

26
00:01:11,450 --> 00:01:13,549
So, if we train a model to minimize error,

27
00:01:13,549 --> 00:01:16,670
it will build a boundary like the one on the right, not the one on the left.

28
00:01:16,670 --> 00:01:21,060
So the question is, how do we pick the one in the left? Well, here's an idea.

29
00:01:21,060 --> 00:01:24,200
Let's look at the equation and let's say the equation of the line on the left is

30
00:01:24,200 --> 00:01:27,965
something like 3x_1 plus 4x_2 plus five equals zero.

31
00:01:27,965 --> 00:01:31,159
The equation of the polynomial is something more complex with

32
00:01:31,159 --> 00:01:35,834
high degree terms like x_1 squared x_1 x_2 x_2 cube, et cetera.

33
00:01:35,834 --> 00:01:37,779
If we look at the equation in the left,

34
00:01:37,780 --> 00:01:40,049
it's much simpler than the equation on the right.

35
00:01:40,049 --> 00:01:42,935
In particular, there are less coefficients, only three,

36
00:01:42,935 --> 00:01:46,490
four, five whereas the right one has many more.

37
00:01:46,489 --> 00:01:50,509
So, if we find a way to in commend the error by some function of these numbers,

38
00:01:50,510 --> 00:01:52,805
that would be very helpful because in some way

39
00:01:52,805 --> 00:01:56,170
the complexity of the model will be added into the error.

40
00:01:56,170 --> 00:01:59,629
So a complex model will have a larger error and then a simple model.

41
00:01:59,629 --> 00:02:03,439
So, let's do that and I'll show you the details later but the idea is that

42
00:02:03,439 --> 00:02:07,349
we take this three and four and notice that we're forgetting about the constant term,

43
00:02:07,349 --> 00:02:08,689
and there's a reason for that.

44
00:02:08,689 --> 00:02:11,699
But if we take this three and four and say add them to the error,

45
00:02:11,699 --> 00:02:13,644
we get a slightly bigger error.

46
00:02:13,645 --> 00:02:17,200
But what if we take all these coefficients and add them to the error here,

47
00:02:17,199 --> 00:02:18,629
now we get a huge error.

48
00:02:18,629 --> 00:02:21,319
Now, we can see that the modeling the left is better

49
00:02:21,319 --> 00:02:23,959
because it has a smaller combined error, so again,

50
00:02:23,960 --> 00:02:26,980
what we did is we took the complexity in the model into account,

51
00:02:26,979 --> 00:02:29,719
when we calculated the error and in that way,

52
00:02:29,719 --> 00:02:32,724
a simpler model has an edge over the complicated model.

53
00:02:32,724 --> 00:02:36,989
Simpler models have a tendency to generalize better so, that's what we want.

54
00:02:36,990 --> 00:02:39,610
So now, let me be more detailed on how to take

55
00:02:39,610 --> 00:02:42,605
the complexity of a model and turn into part of the error.

56
00:02:42,604 --> 00:02:48,489
In summary, will take this highlighted coefficients and somehow add them to the error,

57
00:02:48,490 --> 00:02:52,875
this method is called L1 regularization and it's very simple, here's how it works.

58
00:02:52,875 --> 00:02:54,810
What L1 regularization does,

59
00:02:54,810 --> 00:02:58,930
it takes the coefficient and just adds the absolute values of them to the error.

60
00:02:58,930 --> 00:03:01,599
So in this case, we're adding absolute value

61
00:03:01,599 --> 00:03:04,039
of two which is two plus absolute value of minus two,

62
00:03:04,039 --> 00:03:07,704
which is two again, et cetera and we see that they add to 21.

63
00:03:07,705 --> 00:03:09,260
In the linear case,

64
00:03:09,259 --> 00:03:13,349
we see that we're adding is the absolute value of three and four which is seven,

65
00:03:13,349 --> 00:03:15,364
so a seven is much less than 21,

66
00:03:15,365 --> 00:03:18,920
we can see that the complicated model gives us a much higher error.

67
00:03:18,919 --> 00:03:23,769
That's L1 regularization and the one is attached to the absolute value.

68
00:03:23,770 --> 00:03:25,810
L2 regularization is very similar and what we do

69
00:03:25,810 --> 00:03:28,344
here is instead of adding the absolute values,

70
00:03:28,344 --> 00:03:30,830
we add the squares of the coefficients.

71
00:03:30,830 --> 00:03:32,420
So for the complicated case,

72
00:03:32,419 --> 00:03:34,449
we get two squared plus minus two squared,

73
00:03:34,449 --> 00:03:36,935
et cetera which gives us 85.

74
00:03:36,935 --> 00:03:41,020
For the linear case, we get three squared plus four squared which is 25,

75
00:03:41,020 --> 00:03:43,145
which is much smaller than 85.

76
00:03:43,145 --> 00:03:47,195
So again, we see that the complex model gets punished a lot more than the simple model.

77
00:03:47,194 --> 00:03:48,574
But now the question is,

78
00:03:48,574 --> 00:03:53,019
what if we punish the complicated model too little or what if we punish it too much?

79
00:03:53,020 --> 00:03:57,280
Maybe some models, like a model to send a rocket to the moon or a medical model,

80
00:03:57,280 --> 00:04:01,099
have very little room for error and we're okay with some complexity,

81
00:04:01,099 --> 00:04:03,849
or maybe other models like a video recommendation model,

82
00:04:03,849 --> 00:04:07,210
or model recommending potential friends on a social network have more room

83
00:04:07,210 --> 00:04:11,000
for experimenting and need to be simpler and faster to run a big data.

84
00:04:11,000 --> 00:04:12,590
So we're okay with having some error.

85
00:04:12,590 --> 00:04:14,715
So it seems that for every case,

86
00:04:14,715 --> 00:04:18,720
we have to tune how much we want to punish complexity in each model.

87
00:04:18,720 --> 00:04:22,715
This can be fixed with a parameter and this parameter is called lambda.

88
00:04:22,714 --> 00:04:24,109
What we do with lambda,

89
00:04:24,110 --> 00:04:27,845
is we multiply the complexity part of the error as follows.

90
00:04:27,845 --> 00:04:29,870
Let's look at the two models again and let's

91
00:04:29,870 --> 00:04:31,910
remember that the yellow part of the error comes from

92
00:04:31,910 --> 00:04:36,115
the misclassified points and the green part comes from the complexity of the model,

93
00:04:36,115 --> 00:04:38,795
namely the coefficients in the polynomial.

94
00:04:38,795 --> 00:04:40,720
Let's say, we have a small lambda,

95
00:04:40,720 --> 00:04:45,845
so we take the green error and multiplied by small lambda which gives us something small.

96
00:04:45,845 --> 00:04:48,550
Therefore, the right model still wins because

97
00:04:48,550 --> 00:04:51,910
the complexity part of the error is small and it won't swing the balance.

98
00:04:51,910 --> 00:04:54,610
But if we have a large value for lambda,

99
00:04:54,610 --> 00:04:57,895
then we're multiplying the complexity part of the error by a lot.

100
00:04:57,894 --> 00:05:02,319
Which punishes the complex model more and then the simple model wins.

101
00:05:02,319 --> 00:05:04,149
So in summary, this is what happens,

102
00:05:04,149 --> 00:05:05,949
if we have a large lambda then we're punishing

103
00:05:05,949 --> 00:05:08,964
complexity by a large amount and we're picking a simpler model.

104
00:05:08,964 --> 00:05:10,719
Whereas if we have a small lambda,

105
00:05:10,720 --> 00:05:12,870
then we're punishing complexity by a small amount,

106
00:05:12,870 --> 00:05:15,689
so we're okay with having more complex models.

107
00:05:15,689 --> 00:05:19,949
Now the question is, which one to use L1 or L2 regularization?

108
00:05:19,949 --> 00:05:23,259
So here's a cheat sheet with some benefits for each one.

109
00:05:23,259 --> 00:05:26,259
L1 regularization is actually computationally inefficient

110
00:05:26,259 --> 00:05:29,230
even though it seems simpler because it has no squares,

111
00:05:29,230 --> 00:05:31,660
but actually those absolute values are hard to differentiate.

112
00:05:31,660 --> 00:05:35,439
Whereas, an L2 regularization squares have very nice derivatives.

113
00:05:35,439 --> 00:05:37,834
So, these are easy to deal with computation.

114
00:05:37,834 --> 00:05:40,629
The only times where L1 regularization is faster than

115
00:05:40,629 --> 00:05:43,555
L2 regularization is when the data is sparse.

116
00:05:43,555 --> 00:05:45,949
So let's say if you have a thousand columns of data but

117
00:05:45,949 --> 00:05:48,334
only 10 are relevant and the rest are mostly zeros,

118
00:05:48,334 --> 00:05:49,759
then L1 is faster,

119
00:05:49,759 --> 00:05:52,420
L2 is better for non-sparse outputs which

120
00:05:52,420 --> 00:05:55,290
is when the data is more equally distributed among the columns.

121
00:05:55,290 --> 00:05:57,280
L1 has one huge benefit which is that,

122
00:05:57,279 --> 00:05:58,754
it gives us feature selection.

123
00:05:58,754 --> 00:06:00,034
So let's say, we have again,

124
00:06:00,035 --> 00:06:01,840
data in a thousand columns but really only

125
00:06:01,839 --> 00:06:04,139
10 of them matters and the rest are mostly noise.

126
00:06:04,139 --> 00:06:08,169
So, L1 will detect this and will make the relevant columns into zeroes.

127
00:06:08,170 --> 00:06:12,715
L2 on the other hand won't do this and it just take the columns and treat them similarly.

128
00:06:12,714 --> 00:06:14,919
So that's it, that's regularization.

