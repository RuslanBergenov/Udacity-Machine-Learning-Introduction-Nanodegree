1
00:00:00,750 --> 00:00:05,030
我们已经了解到 类似图中的简单神经网络

2
00:00:05,030 --> 00:00:05,910
输出数据的具体过程

3
00:00:05,910 --> 00:00:09,690
我们构建神经网络是为了输出预测结果

4
00:00:09,690 --> 00:00:14,030
但若提前不知道正确权重 应当怎样处理呢？

5
00:00:14,030 --> 00:00:17,530
我们可以先输入已知的正确数据

6
00:00:17,530 --> 00:00:21,640
然后根据预测结果来调整权重参数

7
00:00:21,640 --> 00:00:25,330
首先 我们需要选取衡量预测误差的标准

8
00:00:25,330 --> 00:00:28,500
最容易想到的是 用实际目标值 y 减去

9
00:00:28,500 --> 00:00:32,220
网络输出值 ŷ 以两者差值衡量误差

10
00:00:32,220 --> 00:00:36,090
然而 若预测值高于目标值 差值为负数

11
00:00:36,090 --> 00:00:40,270
若预测值低于目标值 差值为正数

12
00:00:40,270 --> 00:00:42,370
我们希望误差能够保持符号一致

13
00:00:43,420 --> 00:00:46,890
要让符号全部归正 可以求差值的平方

14
00:00:46,890 --> 00:00:50,630
你可能在想 为什么不直接用绝对值呢

15
00:00:50,630 --> 00:00:53,250
使用平方值时 异常数值会被赋予更高的惩罚值

16
00:00:53,250 --> 00:00:56,020
而较小误差的惩罚值则较低

17
00:00:56,020 --> 00:00:59,690
而且使用平方值还能简化后续计算

18
00:00:59,690 --> 00:01:02,455
但目前我们仅得到单次预测的误差

19
00:01:02,455 --> 00:01:06,860
我们希望求出全体数据的整体误差

20
00:01:06,860 --> 00:01:12,000
那么可以对每项数据 (μ) 的误差求和

21
00:01:13,010 --> 00:01:15,230
符号表示为 ∑ 下标 μ 其形似英文字母 u

22
00:01:15,230 --> 00:01:20,190
这样就得到了全体数据的整体误差

23
00:01:20,190 --> 00:01:20,990
最后

24
00:01:20,990 --> 00:01:25,100
我们在式子前面加上 ½ 以便简化后续计算

25
00:01:25,100 --> 00:01:29,010
此公式通常称为误差平方和

26
00:01:29,010 --> 00:01:34,550
简称 SSE 计算方法就是对差值取平方再求和

27
00:01:35,590 --> 00:01:38,910
还记得 ŷ 等于权重值和输入值的线性组合

28
00:01:38,910 --> 00:01:41,780
再传入激活函数所得的结果

29
00:01:41,780 --> 00:01:45,240
将其代入公式 可以看到误差取决于

30
00:01:45,240 --> 00:01:48,500
权重值 wᵢ 和输入值 xᵢ

31
00:01:49,580 --> 00:01:53,850
刚才讲过 全体数据用希腊字母 μ 表示

32
00:01:53,850 --> 00:01:57,360
你可以将这些数据看成两组表格 数组 或者矩阵

33
00:01:57,360 --> 00:01:59,180
只要便于你理解即可

34
00:01:59,180 --> 00:02:04,360
其中一组包含输入值 x 另一组包含目标值 y

35
00:02:04,360 --> 00:02:07,940
每项数据占一行位置 μ = 1 表示第一行数据

36
00:02:08,940 --> 00:02:11,160
如果需要计算整体误差

37
00:02:11,160 --> 00:02:15,590
可以逐行计算误差平方和

38
00:02:15,590 --> 00:02:17,440
然后对所得结果求和

39
00:02:18,580 --> 00:02:21,800
误差平方和 可用于衡量神经网络的预测效果

40
00:02:21,800 --> 00:02:24,540
值越高 预测效果越差

41
00:02:24,540 --> 00:02:27,800
值越低 预测效果越好

42
00:02:27,800 --> 00:02:30,700
因此我们希望尽量降低误差平方和

43
00:02:30,700 --> 00:02:35,220
下面举个简化示例 暂时仅考虑单行数据的情况

44
00:02:35,220 --> 00:02:38,620
以便于我们理解误差最小化的过程

45
00:02:38,620 --> 00:02:43,530
在这个简单神经网络中 误差平方和等于

46
00:02:43,530 --> 00:02:46,680
目标值减去预测值 (y - ŷ) 取平方除以 2 的结果

47
00:02:46,680 --> 00:02:51,440
在展开预测值之后 可以看出 权重是误差函数的参数

48
00:02:51,440 --> 00:02:54,880
因此权重可被当作控制旋钮 用来调整预测值

49
00:02:54,880 --> 00:02:57,930
从而最终影响整体误差

50
00:02:57,930 --> 00:03:00,919
我们的目标是 求取能使误差最小化的权重值

51
00:03:01,950 --> 00:03:05,580
这是单一权重误差函数的简化图形

52
00:03:05,580 --> 00:03:08,790
我们的目标是 求取图形碗底对应的权值

53
00:03:08,790 --> 00:03:10,340
我们从某个随机权值出发

54
00:03:10,340 --> 00:03:14,130
逐步向误差最小值的方向前进

55
00:03:14,130 --> 00:03:18,190
这个方向与梯度（斜率）相反

56
00:03:18,190 --> 00:03:21,580
只要始终沿着梯度 反复逐步下降

57
00:03:21,580 --> 00:03:25,410
最终就能求得对应最小误差的权值

58
00:03:25,410 --> 00:03:27,900
这就是梯度下降的过程

59
00:03:27,900 --> 00:03:32,490
下面我们来更新权值 新的权值 wᵢ 等于

60
00:03:32,490 --> 00:03:34,940
旧的权值 wᵢ 加上 更新步长 Δwᵢ

61
00:03:36,140 --> 00:03:40,070
希腊大写字母 Δ 通常表示变化量

62
00:03:40,070 --> 00:03:43,169
前述更新步长与梯度成正比

63
00:03:43,169 --> 00:03:47,543
而梯度等于 “误差关于每个权重 wᵢ 的偏导数”

64
00:03:47,543 --> 00:03:51,367
公式中还需添加一个缩放系数变量 用来控制

65
00:03:51,367 --> 00:03:53,780
梯度下降中更新步长的大小

66
00:03:53,780 --> 00:03:56,720
这个变量叫作学习速率 用希腊字母 η 表示

67
00:03:57,910 --> 00:04:01,410
计算梯度需要使用到多元微积分

68
00:04:01,410 --> 00:04:04,970
你可能已经意识到这点 毕竟前面已经用到了偏导

69
00:04:04,970 --> 00:04:08,890
如果你不明白数学细节 也没有关系

70
00:04:08,890 --> 00:04:12,540
了解梯度下降的概念和最终结果

71
00:04:12,540 --> 00:04:14,380
相对更加重要

72
00:04:14,380 --> 00:04:18,620
如果你需要复习材料 建议选择可汗学院的多元微积分课程

73
00:04:18,620 --> 00:04:21,130
我会为你附上相关链接

74
00:04:21,130 --> 00:04:24,030
下面我们展开计算梯度 相当于

75
00:04:24,030 --> 00:04:26,970
对误差平方求关于权重的偏导数

76
00:04:26,970 --> 00:04:30,330
鉴于输出值 ŷ 是权重的函数

77
00:04:30,330 --> 00:04:33,820
这里整体相当于一个复合函数

78
00:04:33,820 --> 00:04:35,750
其参数仍是权重

79
00:04:35,750 --> 00:04:39,080
这种情况需要使用链式法则来求偏导

80
00:04:40,210 --> 00:04:43,120
我们来快速复习一下链式法则

81
00:04:43,120 --> 00:04:46,340
假设你要对 p 函数求关于 z 的偏导

82
00:04:46,340 --> 00:04:51,150
其中 p 是 q 的函数 而 q 又是 z 的函数

83
00:04:51,150 --> 00:04:55,442
根据链式法则 先求 p 关于 q 的偏导

84
00:04:55,442 --> 00:04:58,825
然后乘以 q 关于 z 的偏导

85
00:04:58,825 --> 00:05:02,879
这里可以想象成普通分数 分子和分母中的

86
00:05:02,879 --> 00:05:07,535
两个 ∂q 相互抵消 得到 ∂p/∂z

87
00:05:07,535 --> 00:05:11,877
具体到当前问题中 可以将 q 设为预测差值 (y - ŷ)

88
00:05:11,877 --> 00:05:14,811
并将 p 设为误差平方

89
00:05:14,811 --> 00:05:18,842
然后我们逐项来求关于 wᵢ 的偏导

90
00:05:18,842 --> 00:05:23,211
首先 p 关于 q 的偏导等于预测差值本身

91
00:05:23,211 --> 00:05:27,100
这是因为指数 2 可以提下来 与 ½ 抵消

92
00:05:27,100 --> 00:05:31,140
然后再求 预测差值关于 wᵢ 的偏导

93
00:05:32,290 --> 00:05:36,120
其中目标值 y 是常数 而 ŷ 是权重的函数

94
00:05:37,130 --> 00:05:40,200
再次使用链式法则  把 ŷ 前的负号提到最前面

95
00:05:40,200 --> 00:05:43,350
下面再来对 ŷ 求偏导

96
00:05:43,350 --> 00:05:47,650
应当记得 ŷ 等于 h 的激活函数

97
00:05:47,650 --> 00:05:51,810
而 h 是权重值和输入值的线性组合

98
00:05:51,810 --> 00:05:55,520
再次使用链式法则来求 ŷ 的偏导

99
00:05:55,520 --> 00:05:59,010
得到 h 的激活函数的导数

100
00:05:59,010 --> 00:06:02,390
乘以 线性组合的偏导数

101
00:06:02,390 --> 00:06:06,310
在求和式子中 每个权重仅是单个子项的参数

102
00:06:06,310 --> 00:06:07,090
展开求和式子之后 先来看 w₁

103
00:06:07,090 --> 00:06:12,410
可以看出 w₁ 仅是 x₁ 首项的自变量

104
00:06:12,410 --> 00:06:17,180
所以总和关于 w₁ 的偏导就是 x₁

105
00:06:17,180 --> 00:06:19,520
其余项均为 0

106
00:06:19,520 --> 00:06:24,860
因此总和关于 wᵢ 的偏导数等于 xᵢ

107
00:06:24,860 --> 00:06:28,420
综合来看 误差平方关于 wᵢ 的偏导数

108
00:06:28,420 --> 00:06:31,540
等于负的预测差值

109
00:06:31,540 --> 00:06:37,410
乘以 h 的激活函数的导数 再乘以输入值 xᵢ

110
00:06:37,410 --> 00:06:40,820
那么更新步长就等于学习速率 η 乘以预测差值

111
00:06:40,820 --> 00:06:44,590
乘以激活函数导数 再乘以输入值

112
00:06:44,590 --> 00:06:48,730
为方便后续应用 我们将 “预测差值乘以 h 的激活函数的导数”

113
00:06:48,730 --> 00:06:53,250
命名为 “误差项” 符号表示为希腊小写字母 δ

114
00:06:54,360 --> 00:06:58,660
那么权值更新公式可以重新写为 wᵢ 等于 wᵢ 加上

115
00:06:58,660 --> 00:07:03,320
学习速率乘以误差项乘以输入值 xᵢ

116
00:07:04,350 --> 00:07:07,410
你的神经网络中可能会有多个输出单元

117
00:07:07,410 --> 00:07:10,820
可以将其视为单个输出网络的堆叠架构

118
00:07:10,820 --> 00:07:14,469
但需将输入单元连接到新的输出单元

119
00:07:15,820 --> 00:07:19,940
这时整体误差等于每个输出单元的误差之和

120
00:07:20,770 --> 00:07:25,030
梯度下降法可以扩展适用于这种情况

121
00:07:25,030 --> 00:07:27,148
只需分别计算每个输出单元的误差项

122
00:07:27,148 --> 00:07:30,213
符号表示为 δⱼ

123
00:07:30,213 --> 00:07:33,000
下节课中 我会演示如何将本课内容转化成代码

124
00:07:33,000 --> 00:07:35,267
教大家如何使用 Python 和 Numpy 实现梯度下降算法

