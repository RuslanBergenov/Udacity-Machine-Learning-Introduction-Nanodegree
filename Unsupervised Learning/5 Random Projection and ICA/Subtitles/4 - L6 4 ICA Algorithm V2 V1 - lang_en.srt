1
00:00:00,000 --> 00:00:02,109
Welcome back. In this video,

2
00:00:02,109 --> 00:00:06,394
we'll talk a little bit of the Independent Component Analysis Algorithm.

3
00:00:06,394 --> 00:00:08,369
So, this is going to be a very high level view,

4
00:00:08,369 --> 00:00:10,329
we'll not delve deep into the math.

5
00:00:10,330 --> 00:00:11,960
We'll refer you to the right place,

6
00:00:11,960 --> 00:00:15,580
but it's good and valuable that you know.

7
00:00:15,580 --> 00:00:17,789
Let's have a general idea about how it works and

8
00:00:17,789 --> 00:00:21,219
what assumptions are there when we want to use it.

9
00:00:21,219 --> 00:00:24,964
So, the dataset that we have, we call X.

10
00:00:24,964 --> 00:00:29,769
Right? So, that X was generated by multiplying.

11
00:00:29,769 --> 00:00:32,060
What we would call a mixing matrix,

12
00:00:32,060 --> 00:00:34,844
which is A by the source signals,

13
00:00:34,844 --> 00:00:36,140
which we also don't have.

14
00:00:36,140 --> 00:00:38,365
So, we don't have A, we don't have S. But,

15
00:00:38,365 --> 00:00:41,140
S is what we want to calculate in the end.

16
00:00:41,140 --> 00:00:44,734
So, if X is A times S, we can say that S,

17
00:00:44,734 --> 00:00:48,424
which is the source goal of what we want here is W,

18
00:00:48,424 --> 00:00:50,404
which is the inverse of A.

19
00:00:50,405 --> 00:00:52,439
So, if A is the mixing matrix,

20
00:00:52,439 --> 00:00:54,659
we can call W the unmixing the matrix,

21
00:00:54,659 --> 00:00:56,659
times X times the dataset that we have,

22
00:00:56,659 --> 00:00:58,889
the original recordings that we have.

23
00:00:58,890 --> 00:01:00,660
So, this formula here.

24
00:01:00,659 --> 00:01:02,234
So, X is an input that we have,

25
00:01:02,234 --> 00:01:05,269
W is what we are trying to calculate,

26
00:01:05,269 --> 00:01:06,450
S is the results.

27
00:01:06,450 --> 00:01:09,769
So, the ICA algorithm in a process is all about approximating

28
00:01:09,769 --> 00:01:13,759
W or finding the best W that we can multiply by X,

29
00:01:13,760 --> 00:01:17,660
the dataset here to produce the original signals.

30
00:01:17,659 --> 00:01:21,200
The ICA algorithm is explained clearly in

31
00:01:21,200 --> 00:01:25,460
this paper called Independent Component Analysis: Algorithms and Applications.

32
00:01:25,459 --> 00:01:28,129
It goes into the derivation of everything here.

33
00:01:28,129 --> 00:01:32,174
It shows a couple of ways to calculate number of different parts of the algorithm,

34
00:01:32,174 --> 00:01:37,304
but if we're to have a just a very high-level view of the algorithm called FastICA.

35
00:01:37,305 --> 00:01:38,420
So, this is one way,

36
00:01:38,420 --> 00:01:41,140
this is the one that's implemented actually in scikit-learn.

37
00:01:41,140 --> 00:01:43,459
First, we have X which is our dataset.

38
00:01:43,459 --> 00:01:46,069
We'll center it, we can whiten it, and then,

39
00:01:46,069 --> 00:01:49,129
we will choose an initial random weights matrix.

40
00:01:49,129 --> 00:01:51,179
We'll call that W. In third step,

41
00:01:51,180 --> 00:01:55,180
we estimate W and W as a matrix contains vectors.

42
00:01:55,180 --> 00:01:57,875
The number of vectors, each one is as a weight vector.

43
00:01:57,875 --> 00:02:02,355
After we estimate it, we decorrelate it and the correlation is

44
00:02:02,355 --> 00:02:07,165
to prevent W1 and W2 to convert to the same values.

45
00:02:07,165 --> 00:02:10,010
So, we want them to converge to different values.

46
00:02:10,009 --> 00:02:14,069
Then, we repeat from three, until we converge.

47
00:02:14,069 --> 00:02:17,120
Until we find a value of W that we're satisfied with.

48
00:02:17,120 --> 00:02:19,580
Most of them have comes at step number three here.

49
00:02:19,580 --> 00:02:23,450
So, how does the estimation happen?

50
00:02:23,449 --> 00:02:28,264
This is the formula for estimating each of the vectors here.

51
00:02:28,264 --> 00:02:30,709
Where E is the expected value,

52
00:02:30,710 --> 00:02:32,594
X is the dataset,

53
00:02:32,594 --> 00:02:36,104
G is just some non-quadratic function.

54
00:02:36,104 --> 00:02:38,794
We have the ability to choose a number of them.

55
00:02:38,794 --> 00:02:40,599
What is commonly used,

56
00:02:40,599 --> 00:02:43,310
what scikit-learn uses and what the paper proposes?

57
00:02:43,310 --> 00:02:44,990
As one of the options is tanh,

58
00:02:44,990 --> 00:02:51,159
a hyper tangent function and the decorrelation is calculated like this.

59
00:02:51,159 --> 00:02:53,090
So, let's talk a little bit about this.

60
00:02:53,090 --> 00:02:55,020
So, since it might seem a little bit cryptic.

61
00:02:55,020 --> 00:02:57,830
So, ICA assumes a couple of things.

62
00:02:57,830 --> 00:03:01,400
Right? It assumes that the components are statistically

63
00:03:01,400 --> 00:03:06,760
independent and the paper explains a little bit what that means in statistical language.

64
00:03:06,759 --> 00:03:11,914
It also assumes that components must have non-Gaussian distributions.

65
00:03:11,914 --> 00:03:14,939
Non-Gaussianity is very important here.

66
00:03:14,939 --> 00:03:18,419
It's actually the key to estimating ICA and without it,

67
00:03:18,419 --> 00:03:19,939
we'll not be able to calculate,

68
00:03:19,939 --> 00:03:23,844
we'll not be able to restore the original signals if they were Gaussian.

69
00:03:23,844 --> 00:03:25,504
So, building from here,

70
00:03:25,504 --> 00:03:27,609
the central limit theorem tells us that

71
00:03:27,610 --> 00:03:31,705
the distribution of a sum of independent variables,

72
00:03:31,705 --> 00:03:34,570
tends towards a Gaussian distribution.

73
00:03:34,569 --> 00:03:36,935
So, knowing that, we take W,

74
00:03:36,935 --> 00:03:38,539
this weight matrix here.

75
00:03:38,539 --> 00:03:45,034
We take it to be a matrix that maximizes the non-Gaussianity of W transpose X.

76
00:03:45,034 --> 00:03:48,329
So, non-Gaussianity strikes is here again.

77
00:03:48,330 --> 00:03:51,600
But here, is exactly we have to calculate non-Gaussianity,

78
00:03:51,599 --> 00:03:55,659
because that's the term that this entire algorithm tries to maximize.

79
00:03:55,659 --> 00:03:59,710
So, what's one way to calculate non-Gaussianity?

80
00:03:59,710 --> 00:04:03,930
This term here, is an approximation of something called negentropy.

81
00:04:03,930 --> 00:04:08,230
Negentropy is comes from information theory,

82
00:04:08,229 --> 00:04:13,609
where the idea of entropy comes from and this is a way to approximate it.

83
00:04:13,610 --> 00:04:15,395
You don't need to know all these details,

84
00:04:15,395 --> 00:04:18,819
as long as you know the assumptions of non-Gaussianity.

85
00:04:18,819 --> 00:04:23,139
The independent components, they have to be independent or else I say,

86
00:04:23,139 --> 00:04:24,829
we'll not be able to find them.

87
00:04:24,829 --> 00:04:26,664
If you know these conditions,

88
00:04:26,665 --> 00:04:31,050
and you know the situation where ICA needs to work, that's important.

89
00:04:31,050 --> 00:04:35,129
If you want to know the details of and they look at the derivation,

90
00:04:35,129 --> 00:04:38,759
that paper is link in the comments below and they description below the video.

