1
00:00:00,000 --> 00:00:02,767
Agora estamos lidando
com múltiplas camadas,

2
00:00:02,801 --> 00:00:06,033
e queremos treinar a rede
com descida do gradiente.

3
00:00:06,067 --> 00:00:08,701
Já sabemos calcular o erro
no nó de saída.

4
00:00:08,734 --> 00:00:11,133
Podemos usar este erro
com descida do gradiente

5
00:00:11,167 --> 00:00:13,400
para treinar os pesos
da oculta para a saída.

6
00:00:13,434 --> 00:00:15,968
Mas para treinar os pesos
da entrada para a oculta,

7
00:00:16,000 --> 00:00:19,367
precisamos saber o erro
das unidades da camada oculta.

8
00:00:19,400 --> 00:00:20,767
Como encontrar esses erros

9
00:00:20,801 --> 00:00:22,701
para usar no passo
da descida do gradiente?

10
00:00:22,734 --> 00:00:26,400
Antes encontramos os erros usando
a derivada dos erros ao quadrado

11
00:00:26,434 --> 00:00:29,601
em relação aos pesos entre
as camadas de entrada e saída.

12
00:00:29,634 --> 00:00:31,000
Se fizermos isso
com a camada oculta,

13
00:00:31,033 --> 00:00:32,367
usando a regra da cadeia,

14
00:00:32,400 --> 00:00:34,534
temos que o erro
para as suas unidades

15
00:00:34,567 --> 00:00:36,767
é proporcional ao erro
na camada de saída

16
00:00:36,801 --> 00:00:39,200
vezes o erro
entre as unidades.

17
00:00:39,234 --> 00:00:40,234
Isso faz sentido.

18
00:00:40,267 --> 00:00:42,767
A unidade com conexão mais forte
ao nó de saída

19
00:00:42,801 --> 00:00:45,834
vai contribuir mais erro
para a saída final.

20
00:00:45,868 --> 00:00:48,367
Aqui estamos vendo o erro
vezes os pesos.

21
00:00:48,400 --> 00:00:51,367
É igual à propagação
das entradas pela da rede.

22
00:00:51,400 --> 00:00:54,067
As entradas vezes os pesos
entre as camadas.

23
00:00:54,100 --> 00:00:56,601
Em vez de propagar
as entradas para a frente,

24
00:00:56,634 --> 00:00:59,801
está propagando o erro
para trás pela rede.

25
00:00:59,834 --> 00:01:03,567
Pode ver este processo como
virar a rede ao contrário

26
00:01:03,601 --> 00:01:06,133
e usar o erro como entrada.

27
00:01:06,167 --> 00:01:08,634
Este método se chama
retropropagação.

28
00:01:08,667 --> 00:01:11,300
O processo é o mesmo
quando coloca mais camadas.

29
00:01:11,334 --> 00:01:14,133
Continua propagando o erro
pelas camadas.

30
00:01:14,167 --> 00:01:17,567
Retropropagação é fundamental
a como as redes neurais aprendem.

31
00:01:17,601 --> 00:01:19,400
É muito importante entender

32
00:01:19,434 --> 00:01:21,801
se vai construir modelos
de aprendizagem profunda.

