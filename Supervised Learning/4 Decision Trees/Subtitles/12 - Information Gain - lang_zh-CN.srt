1
00:00:00,000 --> 00:00:03,524
没错 第一种分法完全没用

2
00:00:03,524 --> 00:00:05,370
分割后的数据与原数据都包含相似的红点和蓝点

3
00:00:05,370 --> 00:00:08,085
对于我们了解数据没有帮助

4
00:00:08,085 --> 00:00:09,915
第二种分法就比较好了

5
00:00:09,914 --> 00:00:11,819
我们可以把大多数蓝点分在一边

6
00:00:11,820 --> 00:00:14,669
而把大多数红点分在另一边

7
00:00:14,669 --> 00:00:16,980
所以 我们对于数据有了一定了解

8
00:00:16,980 --> 00:00:18,690
第三种分法就更棒了

9
00:00:18,690 --> 00:00:20,490
它把所有的蓝点分在一边

10
00:00:20,489 --> 00:00:23,039
并把所有的红点分在另一边

11
00:00:23,039 --> 00:00:24,914
现在 我们对于数据的了解就更多了

12
00:00:24,914 --> 00:00:28,750
接下来 我们将学习如何计算信息增益

13
00:00:28,750 --> 00:00:31,300
第一种分法的信息增益为 0 

14
00:00:31,300 --> 00:00:34,299
第二种为 0.28 第三种为 1

15
00:00:34,299 --> 00:00:36,820
信息增益的计算公式很简单

16
00:00:36,820 --> 00:00:38,500
等于熵的变化值

17
00:00:38,500 --> 00:00:40,203
更具体点说

18
00:00:40,203 --> 00:00:42,190
在决策树中的每一个结点处

19
00:00:42,189 --> 00:00:45,714
我们可以计算父结点处数据的熵

20
00:00:45,715 --> 00:00:49,630
然后计算两个子结点的熵

21
00:00:49,630 --> 00:00:52,630
父结点的熵与子结点熵平均值之间的差值

22
00:00:52,630 --> 00:00:56,890
即为信息增益

23
00:00:56,890 --> 00:00:58,420
因此 在第二个例子中

24
00:00:58,420 --> 00:01:01,454
我们计算得到其父结点的熵为 1

25
00:01:01,454 --> 00:01:04,504
各子结点的熵为 0.72

26
00:01:04,504 --> 00:01:08,342
故子结点的熵的平均值也为 0.72

27
00:01:08,343 --> 00:01:14,060
因此 熵的变化值为 1 减 0.72 即0.28

28
00:01:14,060 --> 00:01:18,125
而在这个例子中 我们可以计算得到各自子结点的熵均为 1

29
00:01:18,125 --> 00:01:22,099
因此 熵的变化为 1 减 1 即 0

30
00:01:22,099 --> 00:01:25,399
这个分法很糟糕 因为它没有提供任何信息

31
00:01:25,400 --> 00:01:26,870
而在第三种分法中

32
00:01:26,870 --> 00:01:29,000
两个子结点的熵均为零

33
00:01:29,000 --> 00:01:30,349
正如我们所见

34
00:01:30,349 --> 00:01:33,664
一个集合中点的颜色一致时 其熵为 0

35
00:01:33,665 --> 00:01:37,315
因此 其信息增益为 1 减 0 即 1

36
00:01:37,314 --> 00:01:41,174
这种分法给我们的信息增益最大

37
00:01:41,174 --> 00:01:43,784
它完美地将蓝点和红点分开

38
00:01:43,784 --> 00:01:47,054
因此这是最好的分法

39
00:01:47,055 --> 00:01:51,116
我们做一下总结 以下是三种分法以及其信息增益值

40
00:01:51,115 --> 00:01:52,694
如果决策树必须做出选择

41
00:01:52,694 --> 00:01:54,389
它将选择第三种分法

42
00:01:54,390 --> 00:01:58,200
因为第三种分法提供的信息增益最大

