1
00:00:00,042 --> 00:00:01,447
Bem-vindo de volta.

2
00:00:01,480 --> 00:00:03,521
Neste vídeo,
vamos falar um pouco

3
00:00:03,554 --> 00:00:06,774
sobre algoritmos de análise
de componentes independentes.

4
00:00:06,807 --> 00:00:08,580
Essa será uma visão
bem avançada,

5
00:00:08,613 --> 00:00:10,974
mas não vamos analisar
a matemática a fundo.

6
00:00:11,007 --> 00:00:12,952
Vamos indicar um bom lugar
para isso.

7
00:00:12,985 --> 00:00:15,763
Mas é bom e importante
que você compreenda

8
00:00:15,796 --> 00:00:17,679
a ideia geral
de como isso funciona

9
00:00:17,712 --> 00:00:20,930
e quais são as suposições
que queremos usar.

10
00:00:20,963 --> 00:00:25,670
Vamos chamar o conjunto de dados
que temos de X.

11
00:00:25,703 --> 00:00:29,760
O X foi gerado
pela multiplicação

12
00:00:29,793 --> 00:00:32,491
do que chamaríamos
de "matriz de mistura",

13
00:00:32,524 --> 00:00:34,794
que é A,
com as fontes de sinais,

14
00:00:34,827 --> 00:00:36,444
que também não temos.

15
00:00:36,477 --> 00:00:38,112
Não temos A, não temos S,

16
00:00:38,145 --> 00:00:40,978
mas S é o que queremos
calcular no final das contas.

17
00:00:41,011 --> 00:00:44,858
Se X = AS,
podemos dizer que S,

18
00:00:44,891 --> 00:00:48,501
a fonte do que queremos aqui,
é W,

19
00:00:48,534 --> 00:00:50,657
que é o contrário de A.

20
00:00:50,690 --> 00:00:52,342
Se A é a matriz de mistura,

21
00:00:52,375 --> 00:00:54,637
podemos chamar W
de matriz de não mistura

22
00:00:54,670 --> 00:00:56,375
vezes X,
vezes o conjunto de dados,

23
00:00:56,408 --> 00:00:58,504
as gravações originais
que temos.

24
00:00:58,537 --> 00:01:00,643
Esta fórmula aqui.

25
00:01:00,676 --> 00:01:02,331
X é um input que temos,

26
00:01:02,364 --> 00:01:05,086
W é o que estamos
tentando calcular,

27
00:01:05,119 --> 00:01:06,549
S são os resultados.

28
00:01:06,582 --> 00:01:09,917
O algoritmo da ICA em um processo
serve para aproximar W

29
00:01:09,950 --> 00:01:14,031
ou encontrar o melhor W possível,
multiplicando por X os dados

30
00:01:14,064 --> 00:01:18,057
para produzir
os sinais originais.

31
00:01:18,090 --> 00:01:21,786
O algoritmo da ICA está explicado
em um trabalho chamado

32
00:01:21,819 --> 00:01:23,699
"Análise de Componentes
Independentes:

33
00:01:23,732 --> 00:01:25,481
Algoritmos e Aplicações".

34
00:01:25,514 --> 00:01:28,089
Fala da derivação
de tudo isso aqui.

35
00:01:28,122 --> 00:01:32,067
Mostra algumas maneiras de calcular
várias partes do algoritmo,

36
00:01:32,100 --> 00:01:35,470
mas temos que ter uma visão
bem avançada do algoritmo,

37
00:01:35,503 --> 00:01:37,535
chamada FastICA.

38
00:01:37,568 --> 00:01:38,860
Essa é uma maneira,

39
00:01:38,893 --> 00:01:41,170
a única que está implementada
na scikit-learn.

40
00:01:41,203 --> 00:01:43,770
Primeiro, temos X,
que é o conjunto de dados.

41
00:01:43,803 --> 00:01:45,854
Vamos centralizá-lo,
branqueá-lo,

42
00:01:45,887 --> 00:01:49,123
e vamos escolher uma matriz
com pesos iniciais aleatórios.

43
00:01:49,156 --> 00:01:50,680
Chamamos de W.

44
00:01:50,713 --> 00:01:52,861
No terceiro passo,
estimamos W.

45
00:01:52,894 --> 00:01:55,168
E W, como matriz,
contém vetores.

46
00:01:55,201 --> 00:01:57,919
O número de vetores,
cada um é um vetor de peso.

47
00:01:57,952 --> 00:02:00,551
Depois de estimá-lo,
nós o decorrelacionamos.

48
00:02:00,584 --> 00:02:04,920
E a decorrelação serve
para prevenir W1 e W2

49
00:02:04,953 --> 00:02:07,363
de converterem
os mesmos valores.

50
00:02:07,396 --> 00:02:09,869
Queremos que eles convirjam
em valores diferentes.

51
00:02:09,902 --> 00:02:14,230
Então repetimos a partir daqui
até convergirmos.

52
00:02:14,263 --> 00:02:17,127
Até encontrarmos um valor de W
que nos satisfaça.

53
00:02:17,160 --> 00:02:19,340
A maioria deles aparece
no passo número 3.

54
00:02:19,373 --> 00:02:23,494
Mas como a estimativa
acontece?

55
00:02:23,527 --> 00:02:28,524
Esta é a fórmula para estimar
cada vetor aqui.

56
00:02:28,557 --> 00:02:30,586
E é o valor esperado,

57
00:02:30,619 --> 00:02:32,742
X é o conjunto de dados,

58
00:02:32,775 --> 00:02:36,014
G é apenas uma função
não quadrática.

59
00:02:36,047 --> 00:02:38,937
Temos a habilidade de escolher
algumas delas.

60
00:02:38,970 --> 00:02:40,646
O que é usado normalmente,

61
00:02:40,679 --> 00:02:43,364
o que a scikit usa
e o que o trabalho propõe

62
00:02:43,397 --> 00:02:47,348
como uma das opções, é tahn,
uma função hipertangente.

63
00:02:47,381 --> 00:02:51,311
E a decorrelação é
calculada assim.

64
00:02:51,344 --> 00:02:53,235
Vamos falar
um pouco mais sobre isso,

65
00:02:53,268 --> 00:02:55,295
já que pode parecer
um pouco complicado.

66
00:02:55,328 --> 00:02:58,261
ICA pressupõe algumas coisas.

67
00:02:58,294 --> 00:03:02,009
Pressupõe que os componentes
são estatisticamente independentes,

68
00:03:02,042 --> 00:03:06,690
e o trabalho explica isso
em linguagem estatística.

69
00:03:06,723 --> 00:03:08,759
Também pressupõe
que os componentes

70
00:03:08,792 --> 00:03:12,263
devam ter distribuições
não gaussianas.

71
00:03:12,296 --> 00:03:14,777
A não gaussianidade
é bem importante aqui.

72
00:03:14,810 --> 00:03:18,212
É a chave para estimar a ICA,
e, sem isso,

73
00:03:18,245 --> 00:03:19,928
não conseguiríamos calcular,

74
00:03:19,961 --> 00:03:22,309
não conseguiríamos recuperar,
os sinais originais

75
00:03:22,342 --> 00:03:23,728
se eles fossem gaussianos.

76
00:03:23,761 --> 00:03:25,329
A partir daqui,

77
00:03:25,362 --> 00:03:27,029
o teorema central do limite

78
00:03:27,062 --> 00:03:31,657
nos diz que distribuição das somas
de variáveis independentes

79
00:03:31,690 --> 00:03:34,566
tende em direção
à distribuição gaussiana.

80
00:03:34,599 --> 00:03:36,931
Sabendo disso, pegamos W,

81
00:03:36,964 --> 00:03:38,744
esta matriz de peso aqui.

82
00:03:38,777 --> 00:03:43,294
Fazemos com que ela maximize
a não gaussianidade de W

83
00:03:43,327 --> 00:03:45,191
transposta em X.

84
00:03:45,224 --> 00:03:48,331
A não gaussianidade
ataca novamente.

85
00:03:48,364 --> 00:03:51,370
Mas aqui temos que calcular
a não gaussianidade

86
00:03:51,403 --> 00:03:55,595
porque é o termo
que o algoritmo tenta maximizar.

87
00:03:55,628 --> 00:03:59,777
Qual é forma de calcular
a não gaussianidade?

88
00:03:59,810 --> 00:04:03,890
Este termo aqui é uma aproximação
de algo chamado "negentropia".

89
00:04:03,923 --> 00:04:08,082
A negentropia vem
da teoria da informação,

90
00:04:08,115 --> 00:04:10,503
a ideia de entropia vem disso,

91
00:04:10,536 --> 00:04:13,398
e é uma maneira
de aproximá-la.

92
00:04:13,431 --> 00:04:15,268
Você não precisa
saber tudo isso,

93
00:04:15,301 --> 00:04:17,187
desde de que saiba
todos os pressupostos

94
00:04:17,220 --> 00:04:18,800
da não gaussianidade.

95
00:04:18,833 --> 00:04:21,757
Os componentes independentes
têm que ser independentes,

96
00:04:21,790 --> 00:04:24,637
senão não conseguimos
encontrá-los.

97
00:04:24,670 --> 00:04:26,691
Se você conhece
essas condições

98
00:04:26,724 --> 00:04:30,231
e conhece a situação
da qual a ICA precisa,

99
00:04:30,264 --> 00:04:31,499
isso é importante.

100
00:04:31,532 --> 00:04:35,041
Se quiser saber
os detalhes disso na derivação,

101
00:04:35,074 --> 00:04:37,005
o link do trabalho
está nos comentários

102
00:04:37,038 --> 00:04:38,211
e na descrição abaixo.

