1
00:00:00,590 --> 00:00:03,080
对于这个多层神经网络

2
00:00:03,080 --> 00:00:05,950
我们仍希望使用梯度下降法对其训练

3
00:00:05,950 --> 00:00:09,330
此前我们已经学过 如何计算输出节点的误差项

4
00:00:09,330 --> 00:00:11,919
借助梯度下降法 我们可以使用此误差项来训练

5
00:00:11,919 --> 00:00:13,539
隐藏层到输出层的权重

6
00:00:13,539 --> 00:00:15,899
但要训练输入层到隐藏层的权重

7
00:00:15,900 --> 00:00:19,690
我们需要知道隐藏层单元对应的误差项

8
00:00:19,690 --> 00:00:22,800
那么如何求取梯度下降步骤所需的误差项呢？

9
00:00:22,800 --> 00:00:26,539
此前 我们通过对误差平方求关于 “输入层到输出层权重” 的偏导

10
00:00:26,539 --> 00:00:29,769
来计算误差项

11
00:00:29,769 --> 00:00:33,850
若加入隐藏层 我们使用链式法则时会发现

12
00:00:33,850 --> 00:00:37,050
隐藏层误差项与输出层误差项成正比

13
00:00:37,049 --> 00:00:38,820
比例系数由两层之间的权重决定

14
00:00:38,820 --> 00:00:40,460
这是有道理的

15
00:00:40,460 --> 00:00:42,789
隐藏层单元与输出节点连接越强

16
00:00:42,789 --> 00:00:46,089
则对最终输出值误差项的影响越大

17
00:00:46,090 --> 00:00:48,430
图中可以看到误差项乘以权重值

18
00:00:48,429 --> 00:00:51,469
这与神经网络输入值的正向传播类似

19
00:00:51,469 --> 00:00:54,299
即输入值乘以层间权重值

20
00:00:54,299 --> 00:00:56,599
相对于输入值的正向传播

21
00:00:56,600 --> 00:01:00,070
这是误差项的反向传播

22
00:01:00,070 --> 00:01:03,929
其实可以将此看作 首先反转神经网络

23
00:01:03,929 --> 00:01:06,340
然后将误差项用作输入值

24
00:01:06,340 --> 00:01:08,329
此方法称为反向传播

25
00:01:08,329 --> 00:01:11,450
即使加深层数 这个过程也是一样的

26
00:01:11,450 --> 00:01:14,299
只需逐层不断传播误差项

27
00:01:14,299 --> 00:01:17,810
反向传播是训练神经网络的基础原理

28
00:01:17,810 --> 00:01:19,629
因此对于构建深度学习模型

29
00:01:19,629 --> 00:01:21,439
理解反向传播至关重要

