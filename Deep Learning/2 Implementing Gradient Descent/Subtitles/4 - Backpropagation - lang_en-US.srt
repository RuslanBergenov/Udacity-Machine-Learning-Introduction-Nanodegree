1
00:00:00,590 --> 00:00:03,080
Now we're dealing with
multiple layers and

2
00:00:03,080 --> 00:00:05,950
we'd still like to train
the network with gradient descent.

3
00:00:05,950 --> 00:00:09,330
From before, we know how to calculate
the error in the output node.

4
00:00:09,330 --> 00:00:11,919
We can use this error with
gradient descent to train

5
00:00:11,919 --> 00:00:13,539
the hidden to output weights.

6
00:00:13,539 --> 00:00:15,899
But to train the input
to hidden weights,

7
00:00:15,900 --> 00:00:19,690
we need to know the error caused
by the units in the hidden layer.

8
00:00:19,690 --> 00:00:22,800
How do we find these errors to
use in the gradient descent step?

9
00:00:22,800 --> 00:00:26,539
Before, we found the errors by taking
the derivative of the squared errors

10
00:00:26,539 --> 00:00:29,769
which respect to the weights between
the input and output layers.

11
00:00:29,769 --> 00:00:33,850
If we do that with a hidden layer, using
the chain rule, we find the error for

12
00:00:33,850 --> 00:00:37,050
units there is proportional to
the error in the output layer

13
00:00:37,049 --> 00:00:38,820
times the weight between the unit.

14
00:00:38,820 --> 00:00:40,460
And this makes sense.

15
00:00:40,460 --> 00:00:42,789
A unit with a stronger
connection to the output node,

16
00:00:42,789 --> 00:00:46,089
is going to contribute more
error to the final output.

17
00:00:46,090 --> 00:00:48,430
Here, you see the error
times the weights.

18
00:00:48,429 --> 00:00:51,469
This is the same way you propagate
inputs through the network,

19
00:00:51,469 --> 00:00:54,299
the inputs times the weights
between the layers.

20
00:00:54,299 --> 00:00:56,599
Instead of propagating
the inputs forward,

21
00:00:56,600 --> 00:01:00,070
you're propagating the error
backwards through the network.

22
00:01:00,070 --> 00:01:03,929
Now, you can view this process
as flipping the network over and

23
00:01:03,929 --> 00:01:06,340
using the error as the input.

24
00:01:06,340 --> 00:01:08,329
This method is called backpropagation.

25
00:01:08,329 --> 00:01:11,450
And this process works the same
when you add more layers.

26
00:01:11,450 --> 00:01:14,299
You just keep propagating
the errors through the layers.

27
00:01:14,299 --> 00:01:17,810
Backpropagation is fundamental
to how neural networks learn.

28
00:01:17,810 --> 00:01:19,629
So it's really important
to understand this,

29
00:01:19,629 --> 00:01:21,439
if you're going to be
building deep learning models

