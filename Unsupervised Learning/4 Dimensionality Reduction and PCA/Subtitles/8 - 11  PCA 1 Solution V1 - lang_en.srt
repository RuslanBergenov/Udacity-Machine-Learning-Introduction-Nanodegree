1
00:00:00,000 --> 00:00:01,360
In the previous video,

2
00:00:01,360 --> 00:00:05,425
you saw an example of working with the MNIST digits data.

3
00:00:05,424 --> 00:00:07,915
A link to that dataset can be found here.

4
00:00:07,915 --> 00:00:11,155
First, it's important to load the necessary libraries.

5
00:00:11,154 --> 00:00:13,750
Then, we'll work through a number of questions.

6
00:00:13,750 --> 00:00:16,440
The first thing to notice is that there's a bunch

7
00:00:16,440 --> 00:00:19,185
of stuff loaded from this helper functions library.

8
00:00:19,184 --> 00:00:24,210
These are essentially a number of functions that you saw in the previous video,

9
00:00:24,210 --> 00:00:25,740
as well as some additional things.

10
00:00:25,739 --> 00:00:27,629
We'll see throughout this notebook.

11
00:00:27,629 --> 00:00:31,554
So, the first thing we need to do is use pandas to read in the dataset,

12
00:00:31,554 --> 00:00:34,335
you can see that it's found at this link here.

13
00:00:34,335 --> 00:00:38,310
You can also take a look at the data by doing a head, tail,

14
00:00:38,310 --> 00:00:43,145
describe, info et cetera to see what it looks and make sure you're comfortable.

15
00:00:43,145 --> 00:00:46,520
Similar to the previous video let's create a vector called y that holds

16
00:00:46,520 --> 00:00:49,680
the label and store all the rest of the pixels in

17
00:00:49,679 --> 00:00:57,484
x. I'm also going to do a fillna with zero like we saw in the previous video, great.

18
00:00:57,484 --> 00:00:59,564
So, it looks like that checks out.

19
00:00:59,564 --> 00:01:05,250
Now, use the show images by digit function from the helper functions to look at the ones,

20
00:01:05,250 --> 00:01:08,170
two, threes or any other value.

21
00:01:08,170 --> 00:01:12,034
By running this this should show us all of the twos in the dataset.

22
00:01:12,034 --> 00:01:15,155
If we change that, we can take a look at the ones.

23
00:01:15,155 --> 00:01:19,030
We could also look at say the eights.

24
00:01:19,030 --> 00:01:21,780
Now that we have had a chance to look through the data,

25
00:01:21,780 --> 00:01:24,320
let's try using this four-step process to fit

26
00:01:24,319 --> 00:01:28,284
a model and score our model by predicting the digits.

27
00:01:28,284 --> 00:01:31,109
So, the model that was used in the previous video,

28
00:01:31,109 --> 00:01:33,015
was just via this function here.

29
00:01:33,015 --> 00:01:35,019
This fit random forest classifier.

30
00:01:35,019 --> 00:01:38,954
But you could also try to grid search and find a better model than this one.

31
00:01:38,954 --> 00:01:41,299
We need to do an in-place equals true to

32
00:01:41,299 --> 00:01:44,084
make sure that the data set actually came out okay.

33
00:01:44,084 --> 00:01:49,774
Then, so here you can see an example of how well our model performs.

34
00:01:49,775 --> 00:01:53,680
Again, around that 94 percent mark you saw in the previous video.

35
00:01:53,680 --> 00:01:58,380
So, the purpose of this entire lesson is to look at PCA.

36
00:01:58,379 --> 00:02:01,879
To replicate the process that you saw in the previous video,

37
00:02:01,879 --> 00:02:05,334
you can use to do-PCA function that was created.

38
00:02:05,334 --> 00:02:08,699
If we run this you can see that it takes in two inputs,

39
00:02:08,699 --> 00:02:11,060
the number of components and the dataset that you

40
00:02:11,060 --> 00:02:14,224
want to move into a fewer number of components.

41
00:02:14,224 --> 00:02:17,569
Then the output is the PCA model as well as

42
00:02:17,569 --> 00:02:21,275
the dataset created by creating a fewer number of features,

43
00:02:21,275 --> 00:02:23,870
it takes the number of features.

44
00:02:23,870 --> 00:02:27,230
I create a model using just two features in the previous video,

45
00:02:27,229 --> 00:02:30,090
but you could try something with more if you wanted to.

46
00:02:30,090 --> 00:02:34,969
We'll pass in our x matrix and then the two things that we get back are

47
00:02:34,969 --> 00:02:40,359
the PCA object and the x matrix.

48
00:02:40,360 --> 00:02:46,280
Now, we have back our PCA object and the reduced feature set down to only two.

49
00:02:46,280 --> 00:02:47,965
The next thing that we can do,

50
00:02:47,965 --> 00:02:50,205
is fit a random forest classifier.

51
00:02:50,205 --> 00:02:53,850
This was one of the functions that was created in the previous video.

52
00:02:53,849 --> 00:02:56,299
You can see that it takes in an x and y and

53
00:02:56,300 --> 00:02:58,880
it prints the confusion matrix and the accuracy.

54
00:02:58,879 --> 00:03:02,560
So, in this case you can see that it doesn't perform very well.

55
00:03:02,560 --> 00:03:06,319
At least not in comparison to the model that uses all of the features.

56
00:03:06,319 --> 00:03:09,319
In the next part we want to see if we can find a reduced number of

57
00:03:09,319 --> 00:03:13,314
features that still allows for an accuracy of greater than 90 percent.

58
00:03:13,314 --> 00:03:18,094
So, let's do four features,

59
00:03:18,094 --> 00:03:22,544
let's just say we start at well we know not to.

60
00:03:22,544 --> 00:03:27,234
So, maybe three maybe we go to like 15.

61
00:03:27,235 --> 00:03:29,425
Sorry I skipped number seven.

62
00:03:29,425 --> 00:03:31,370
So, the next thing we can do is use

63
00:03:31,370 --> 00:03:34,460
the plot components function that you saw on the video,

64
00:03:34,460 --> 00:03:36,125
to see how well the separation works.

65
00:03:36,125 --> 00:03:38,210
So, this is exactly what we saw before.

66
00:03:38,210 --> 00:03:40,224
Now, let's go a little bit further.

67
00:03:40,224 --> 00:03:43,745
In this case we want to see if we can get better separation.

68
00:03:43,745 --> 00:03:47,360
Specifically we want to get better than a 90 percent accuracy with

69
00:03:47,360 --> 00:03:51,900
a fewer number of features than using of 700 plus of them.

70
00:03:51,900 --> 00:03:55,110
Let's first just start out with a range from three to 15.

71
00:03:55,110 --> 00:03:57,790
We can always make that higher later if we need to.

72
00:03:57,789 --> 00:04:03,074
So, I'm going to pass that to the do PCA part.

73
00:04:03,074 --> 00:04:06,454
So, do PCA is actually a function from here.

74
00:04:06,455 --> 00:04:09,020
So, the nice part is that it's not only

75
00:04:09,020 --> 00:04:12,230
printing the accuracy but it also returns the accuracy.

76
00:04:12,229 --> 00:04:15,364
We can tell that because it's next to the outer portion here.

77
00:04:15,365 --> 00:04:19,240
So, for each number of features between three and 15,

78
00:04:19,240 --> 00:04:23,555
what we wanna do is run the do PCA algorithm which will give us back

79
00:04:23,555 --> 00:04:28,925
PCA and the x matrix.

80
00:04:28,925 --> 00:04:34,295
Then what we want to do with that is run the random forest classifier which is this part.

81
00:04:34,295 --> 00:04:37,560
The random forest classifier takes in the x and the y.

82
00:04:37,560 --> 00:04:40,375
So, x PCA and y.

83
00:04:40,375 --> 00:04:44,589
They actually returns the accuracy.

84
00:04:44,589 --> 00:04:47,524
So, we can tell that it returns the accuracy because

85
00:04:47,524 --> 00:04:51,054
of this part right here being returned in the out.

86
00:04:51,055 --> 00:04:56,209
What I'm going to do is I'm going to create an empty list called ACCS.

87
00:04:56,209 --> 00:05:03,944
Then what I want to do is append to that list the accuracy from each of these models.

88
00:05:03,944 --> 00:05:13,099
Then I'm also going to create num features outside of that loop as a list, great.

89
00:05:13,100 --> 00:05:15,340
So, this is going to create a whole bunch of these.

90
00:05:15,339 --> 00:05:18,379
So, this is what happens with three features you

91
00:05:18,379 --> 00:05:21,384
can see we get an accuracy of around 54 percent.

92
00:05:21,384 --> 00:05:25,925
Then with four features we get an accuracy around 67 percent.

93
00:05:25,925 --> 00:05:30,235
Then with five features we get 72 percent or so.

94
00:05:30,235 --> 00:05:36,030
Then, around six we get close to 80 percent.

95
00:05:36,029 --> 00:05:38,304
You can see we're starting to level off a little bit.

96
00:05:38,305 --> 00:05:40,879
Then, we get 84, 85,

97
00:05:40,879 --> 00:05:44,310
86 and it looks like it's done running.

98
00:05:44,310 --> 00:05:49,129
So, with 14 features we're up to almost 88 percent.

99
00:05:49,129 --> 00:05:53,120
It looks like we need to go just a little bit higher than that.

100
00:05:53,120 --> 00:06:02,160
So, what I'll do is I'll start at 15 and go to maybe let's say 20 features, okay.

101
00:06:02,160 --> 00:06:07,335
So, with 15 we have 90, were so close.

102
00:06:07,334 --> 00:06:10,404
But it probably would have been smarter to build than a break.

103
00:06:10,404 --> 00:06:14,959
Where we break if the accuracy of this model,

104
00:06:14,959 --> 00:06:19,439
so if we stored this accuracy and then just appended it afterwards.

105
00:06:19,439 --> 00:06:24,875
So, we said like at equals this and then append

106
00:06:24,875 --> 00:06:32,685
the accuracy and then break if acc greater than 0.9.

107
00:06:32,685 --> 00:06:35,360
This probably would've been the smartest way to do it and then just run

108
00:06:35,360 --> 00:06:37,905
it from 15 to, all right.

109
00:06:37,904 --> 00:06:40,209
I mean now we can do it to 100.

110
00:06:40,209 --> 00:06:42,669
But it probably smarter to start at 20.

111
00:06:42,670 --> 00:06:45,080
This part doesn't mean anything down here now.

112
00:06:45,079 --> 00:06:49,539
So, at 20 we have an accuracy of around 90,

113
00:06:49,540 --> 00:06:52,435
it's going back down. There we go.

114
00:06:52,435 --> 00:06:58,459
Now, if we look at insert a cell below, we do print.

115
00:06:58,459 --> 00:07:01,649
We can just look at CCS.

116
00:07:01,649 --> 00:07:03,114
There's 11 of them.

117
00:07:03,115 --> 00:07:05,975
So, you can see that we did 11, yeah.

118
00:07:05,975 --> 00:07:07,500
So, from 20 to 30.

119
00:07:07,500 --> 00:07:12,379
So, basically once we had 30 features that was enough to

120
00:07:12,379 --> 00:07:17,795
understand the digits well enough to predict better than a 90 percent accuracy.

121
00:07:17,795 --> 00:07:23,395
Which is a really big reduction from the 700 plus features that we had before.

122
00:07:23,394 --> 00:07:27,894
It is possible that the extra features actually contribute to overfitting.

123
00:07:27,894 --> 00:07:31,789
Then, this asked do you have evidence that this is happening for this dataset.

124
00:07:31,790 --> 00:07:35,314
In the solution file that you can find in this notebook.

125
00:07:35,314 --> 00:07:39,769
It looks at the range at a pretty wide range,

126
00:07:39,769 --> 00:07:42,964
the suggestion is to look only below 100 components.

127
00:07:42,964 --> 00:07:47,204
So, what we could do is basically run this for 100 components.

128
00:07:47,204 --> 00:07:52,284
Then, plot the accuracy over a number of components and see if it starts dipping or not.

129
00:07:52,285 --> 00:07:54,055
Or CFR model fits well.

130
00:07:54,055 --> 00:07:57,000
But essentially that looks like the same processes up here.

