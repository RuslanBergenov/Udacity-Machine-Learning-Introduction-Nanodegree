1
00:00:00,367 --> 00:00:04,367
Vejamos o que o algoritmo
gradiente descendente faz.

2
00:00:04,401 --> 00:00:07,167
Nós estamos no topo
do Monte Errorest

3
00:00:07,201 --> 00:00:09,234
e precisamos descer.

4
00:00:09,268 --> 00:00:12,367
Para isso, nós damos
um monte de passos

5
00:00:13,467 --> 00:00:16,634
seguindo o negativo
do gradiente da altura,

6
00:00:16,668 --> 00:00:18,400
que é a função de erro.

7
00:00:19,200 --> 00:00:21,267
Cada passo é
chamado de epoch.

8
00:00:21,301 --> 00:00:23,601
Quando nos referimos
à quantidade de passos,

9
00:00:23,635 --> 00:00:25,767
nos referimos
à quantidade de epochs.

10
00:00:25,801 --> 00:00:27,901
Vejamos o que ocorre
em cada epoch.

11
00:00:27,935 --> 00:00:31,667
Nós pegamos a entrada,
todos os nossos dados,

12
00:00:31,701 --> 00:00:34,868
e passamos pela rede neural.

13
00:00:34,902 --> 00:00:36,667
Depois nós encontramos
as previsões,

14
00:00:36,701 --> 00:00:38,334
calculamos o erro,

15
00:00:38,368 --> 00:00:41,767
ou seja, a distância entre eles
e os rótulos,

16
00:00:41,801 --> 00:00:44,300
e fazemos a propagação
de retorno do erro

17
00:00:44,334 --> 00:00:47,801
para atualizar os pesos
na rede neural.

18
00:00:47,835 --> 00:00:51,667
Isso nos dará um limite melhor
para previsão de dados.

19
00:00:51,701 --> 00:00:53,834
Isso é feito
com todos os dados.

20
00:00:53,868 --> 00:00:55,634
Se tivermos
muitos pontos de dados,

21
00:00:55,668 --> 00:00:57,167
que é o que
geralmente acontece,

22
00:00:57,201 --> 00:00:59,567
teremos cálculos matriciais
gigantescos

23
00:00:59,601 --> 00:01:01,701
que usam muita memória.

24
00:01:01,735 --> 00:01:04,000
Tudo isso
para um único passo.

25
00:01:04,034 --> 00:01:07,434
Se houver muitos passos,
imagine o tempo

26
00:01:07,468 --> 00:01:09,133
e a potência de cálculo
necessários.

27
00:01:09,167 --> 00:01:12,067
Há algo que podemos fazer
para acelerar isto?

28
00:01:12,101 --> 00:01:15,868
Nós precisamos ligar
todos os dados

29
00:01:15,902 --> 00:01:17,734
sempre que dermos um passo?

30
00:01:17,768 --> 00:01:19,734
Se os dados estiverem
bem distribuídos,

31
00:01:19,768 --> 00:01:21,601
um pequeno subconjunto

32
00:01:21,635 --> 00:01:24,801
nos dará uma ideia
sobre o que seria o gradiente.

33
00:01:24,835 --> 00:01:27,267
Talvez não seja a melhor
estimativa do gradiente,

34
00:01:27,301 --> 00:01:29,634
mas ele é rápido,
e como estamos iterando,

35
00:01:29,668 --> 00:01:31,434
isso pode ser uma boa ideia.

36
00:01:31,468 --> 00:01:35,434
É aqui que o gradiente
descendente estocástico age.

37
00:01:35,468 --> 00:01:40,267
A ideia por trás disso é pegar
pequenos subconjuntos de dados,

38
00:01:40,301 --> 00:01:42,434
rodá-los pela rede neural,

39
00:01:42,468 --> 00:01:46,200
calcular o gradiente da função
de erro dos pontos

40
00:01:46,234 --> 00:01:48,934
e dar um passo
naquela direção.

41
00:01:48,968 --> 00:01:51,567
Nós ainda queremos usar
todos os dados,

42
00:01:51,601 --> 00:01:53,601
então nós fazemos
o seguinte:

43
00:01:53,635 --> 00:01:56,434
nós dividimos os dados
em vários lotes.

44
00:01:56,468 --> 00:01:59,834
Neste exemplo nós temos
24 pontos.

45
00:01:59,868 --> 00:02:03,334
Nós os dividimos
em quatro lotes de 6 pontos.

46
00:02:03,368 --> 00:02:06,033
Nós pegamos os pontos
do primeiro lote

47
00:02:06,067 --> 00:02:08,434
e os rodamos da rede neural,

48
00:02:08,468 --> 00:02:11,200
calculamos o erro
e o gradiente

49
00:02:11,234 --> 00:02:13,801
e a propagação de retorno
para atualizar os pesos.

50
00:02:13,835 --> 00:02:15,434
Isso nos dará novos pesos,

51
00:02:15,468 --> 00:02:17,334
que definirão
regiões de limites,

52
00:02:17,368 --> 00:02:19,133
que vemos à esquerda.

53
00:02:19,167 --> 00:02:21,934
Pegamos os pontos
do segundo lote

54
00:02:21,968 --> 00:02:24,300
e fazemos o mesmo.

55
00:02:24,334 --> 00:02:26,934
Isso nos dará
pesos melhores

56
00:02:26,968 --> 00:02:29,234
e uma região limite melhor.

57
00:02:29,268 --> 00:02:32,334
Fazemos o mesmo
com o terceiro lote.

58
00:02:32,368 --> 00:02:36,200
Por fim, fazemos
com o quarto.

59
00:02:36,234 --> 00:02:37,701
Pronto.

60
00:02:37,735 --> 00:02:40,868
Nós pegamos os dados,
demos quatro passos,

61
00:02:40,902 --> 00:02:43,801
e, quando fizemos
o gradiente descendente normal,

62
00:02:43,835 --> 00:02:46,634
só demos um passo
com todos os dados.

63
00:02:46,668 --> 00:02:51,033
Os quatro passos são
menos precisos,

64
00:02:51,067 --> 00:02:52,601
mas, na prática,

65
00:02:52,635 --> 00:02:56,267
é melhor dar vários
pequenos passos não precisos

66
00:02:56,301 --> 00:02:58,300
do que dar um bom.

67
00:02:58,334 --> 00:02:59,934
Posteriormente, no curso,

68
00:02:59,968 --> 00:03:02,901
você aplicará o gradiente
descendente estocástico

69
00:03:02,935 --> 00:03:04,634
e verá os benefícios dele.

