1
00:00:00,000 --> 00:00:05,940
Another question you might

2
00:00:05,940 --> 00:00:10,060
have at this point is why did you choose to shrink the points into this line?

3
00:00:10,060 --> 00:00:12,915
Why not a line that looks like this?

4
00:00:12,914 --> 00:00:18,530
For that, we need to understand a bit more about how we choose principal components.

5
00:00:18,530 --> 00:00:22,370
The principle components of a dataset have two properties,

6
00:00:22,370 --> 00:00:25,500
the first property is that you want the components to

7
00:00:25,500 --> 00:00:29,015
capture the largest amount of variance in the dataset.

8
00:00:29,015 --> 00:00:33,310
By choosing components that span the largest variance in the dataset,

9
00:00:33,310 --> 00:00:36,560
you lose the least amount of information.

10
00:00:36,560 --> 00:00:40,915
Visually, you can understand the amount of information that is lost

11
00:00:40,914 --> 00:00:45,850
by looking at the distance from each of the points to the component.

12
00:00:45,850 --> 00:00:48,005
So from our dataset,

13
00:00:48,005 --> 00:00:52,700
let's say we consider these two components the information lost for moving

14
00:00:52,700 --> 00:00:57,734
the data to this component can be seen as the sum of these distances.

15
00:00:57,734 --> 00:00:59,630
If we stack them together,

16
00:00:59,630 --> 00:01:01,935
this is how much information is lost.

17
00:01:01,935 --> 00:01:05,545
Alternatively, let's consider this component.

18
00:01:05,545 --> 00:01:07,680
The amount of information lost,

19
00:01:07,680 --> 00:01:10,965
can be visualized as the sum of these distances.

20
00:01:10,965 --> 00:01:13,700
If we compare the amount of information lost,

21
00:01:13,700 --> 00:01:17,525
we can see that this component loses less information.

22
00:01:17,525 --> 00:01:20,030
The second thing that needs to be true about

23
00:01:20,030 --> 00:01:23,900
our components is that they must be orthogonal to one another.

24
00:01:23,900 --> 00:01:25,950
What exactly does this mean?

25
00:01:25,950 --> 00:01:28,939
So this might take a little review of linear algebra,

26
00:01:28,938 --> 00:01:31,919
but I'll explain the essence of the idea here.

27
00:01:31,920 --> 00:01:36,840
Orthogonal components are those that have 90 degree angles with one another.

28
00:01:36,840 --> 00:01:39,935
These two components here are orthogonal.

29
00:01:39,935 --> 00:01:43,340
Alternatively, two components that do not have

30
00:01:43,340 --> 00:01:48,365
90 degree angles between them are not orthogonal, like these here.

31
00:01:48,364 --> 00:01:51,649
Though all of the examples you've seen so far have been

32
00:01:51,650 --> 00:01:55,410
a two-dimensional data when performing PCA in practice,

33
00:01:55,409 --> 00:02:00,344
it's more likely that you will have many many more dimensions to your data.

34
00:02:00,344 --> 00:02:03,448
Therefore, having the ability to use scikit-learn

35
00:02:03,448 --> 00:02:06,495
to perform these operations will be our next task.

36
00:02:06,495 --> 00:02:07,890
I'll show you how to perform

37
00:02:07,890 --> 00:02:11,729
these operations and then you'll put them to use on your own.

